{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null experiments\n",
    "\n",
    "This notebook includes 2 other experiments conducted on the KuaiRec dataset. They include:\n",
    "- [Matapath2Vec for Non-personalized Recommendation (Autoplay)](#Non-personalized-Recommendation)  \n",
    "    This experiment checks if adjusting the W2V video embeddings with Metapath2Vec can improve non-personalized recommendation performance. Long story short, it does not. Stand alone W2V embeddings are much better for autoplay\n",
    "- [Frozen Metapath2Vec for Personalized Recommendation](#Frozen-Metapath2Vec)  \n",
    "    This experiment checks if it is possible to learn the user embeddings with Metapath2Vec while keeping the video embeddings frozen (same as original W2V video embeddings). This would be attractive because it could allow us to have learned user embeddings while only storing 1 set of video/track embeddings. Long story short, it does not work at all. Unsurprisingly, it is necessary to adjust both video/track and user embeddings jointly to improve personalized recommendation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn.models import MetaPath2Vec\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.index import index2ptr\n",
    "from torch_geometric.typing import EdgeType, NodeType, OptTensor\n",
    "from torch_geometric.utils import sort_edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Non-personalized-Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load KuaiRec user watch histories, filter for only positive interactions (watch_ratio >= 2, as mentioned by the authors),\n",
    "# and create chronological watch sequences for each user (for W2V)\n",
    "train = pd.read_csv('KuaiRec/big_matrix.csv')\n",
    "train = train[train['watch_ratio'] >= 2]\n",
    "train_sequences = train.sort_values(['user_id', 'time'], ascending=[True, True]).groupby('user_id')['video_id'].apply(list).reset_index()\n",
    "train_sequences.to_parquet('KuaiRec/train_sequences.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Take fully observed test set positive interactions & create val and test pairs for non-personalized recommendation\n",
    "# as done in Deezer's W2V Hyperparams matter paper (https://github.com/deezer/w2v_reco_hyperparameters_matter/blob/master/src/main.py)\n",
    "test = pd.read_csv('KuaiRec/small_matrix.csv')\n",
    "test = test[test['watch_ratio'] >= 2].sort_values(['user_id', 'time']).reset_index(drop=True)\n",
    "pairs = test.groupby('user_id')['video_id'].apply(lambda x: list(zip(x[:-1], x[1:]))).explode().tolist()\n",
    "np.random.shuffle(pairs)\n",
    "split_idx = int(len(pairs) * 0.8)\n",
    "test_pairs = pairs[:split_idx]\n",
    "val_pairs = pairs[split_idx:]\n",
    "np.save('KuaiRec/val_pairs.npy', np.array(val_pairs, dtype=int))\n",
    "np.save('KuaiRec/test_pairs.npy', np.array(test_pairs, dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation function for val & test based on Deezer's W2V paper, returns hitrate@k & ndcg@k for KNN recommendations\n",
    "# using the W2V trained embeddings\n",
    "def evaluate(train, test, embedding_dim, window_size, epochs, sg=1, min_count=1, k=20):\n",
    "    model = Word2Vec(\n",
    "            vector_size=embedding_dim,\n",
    "            window=window_size,\n",
    "            workers=multiprocessing.cpu_count(),\n",
    "            sg=sg,\n",
    "            min_count=min_count,\n",
    "            compute_loss=True,\n",
    "        )\n",
    "    model.build_vocab(train)\n",
    "    model.train(\n",
    "            corpus_iterable=train,\n",
    "            total_examples=len(train),\n",
    "            epochs=epochs,\n",
    "        )\n",
    "    vocab = list(model.wv.index_to_key)\n",
    "    embedding = [model.wv[elem] for elem in vocab]\n",
    "    mapping = {elem: i for i, elem in enumerate(vocab)}\n",
    "    mapping_back = {v: k for k, v in mapping.items()}\n",
    "    \n",
    "    neigh = NearestNeighbors()\n",
    "    neigh.fit(embedding)\n",
    "    \n",
    "    hrk_score = 0.0\n",
    "    ndcg_score = 0.0\n",
    "    for pair_items in tqdm(test):\n",
    "        if str(pair_items[0]) not in vocab:\n",
    "            continue\n",
    "        emb_0 = embedding[mapping[str(pair_items[0])]].reshape(1, -1)\n",
    "        # Get neighbors\n",
    "        emb_neighbors = neigh.kneighbors(emb_0, k+1)[1].flatten()[1:]\n",
    "        neighbors = [mapping_back[x] for x in emb_neighbors]\n",
    "        if str(pair_items[1]) in neighbors:\n",
    "            # HR@k\n",
    "            hrk_score += 1/k\n",
    "            # NDCG@k\n",
    "            # In our case only one item in the retrieved list can be relevant,\n",
    "            # so in particular the ideal ndcg is 1 and ndcg_at_k = 1/log_2(1+j)\n",
    "            # where j is the position of the relevant item in the list.\n",
    "            index_match = (np.where(str(pair_items[1]) == np.array(neighbors)))[0][0]\n",
    "            ndcg_score += 1/np.log2(np.arange(2, k+2))[index_match]\n",
    "    hrk_score = hrk_score / len(test)\n",
    "    ndcg_score = ndcg_score / len(test)\n",
    "\n",
    "    return {'HR@%i' % k: 1000*hrk_score, 'NDCG@%i' % k: ndcg_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_parquet('KuaiRec/train_sequences.parquet')\n",
    "sequences = train['video_id'].apply(lambda x: list(map(str, x))).tolist()\n",
    "val = np.load('KuaiRec/val_pairs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Best W2V non-personalized recommendation test set performance\n",
    "test = np.load('KuaiRec/test_pairs.npy')\n",
    "evaluate(sequences, test, 50, 15, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Best W2V video/track embeddings config will be used as initialization for Metapath2vec\n",
    "train = pd.read_parquet('KuaiRec/train_sequences.parquet')\n",
    "sequences = train['video_id'].apply(lambda x: list(map(str, x))).tolist()\n",
    "val = np.load('KuaiRec/val_pairs.npy')\n",
    "model = Word2Vec(\n",
    "            vector_size=50,\n",
    "            window=15,\n",
    "            workers=multiprocessing.cpu_count(),\n",
    "            sg=1,\n",
    "            min_count=1,\n",
    "            compute_loss=True,\n",
    "        )\n",
    "model.build_vocab(sequences)\n",
    "model.train(\n",
    "            corpus_iterable=sequences,\n",
    "            total_examples=len(sequences),\n",
    "            epochs=75,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_watch = pd.read_csv('KuaiRec/big_matrix.csv')\n",
    "\n",
    "unique_users = df_watch['user_id'].unique()\n",
    "unique_videos = df_watch['video_id'].unique()\n",
    "\n",
    "df_watch = df_watch[df_watch['watch_ratio'] >= 2].sort_values(['user_id', 'time'])\n",
    "\n",
    "user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
    "video2idx = {vid: i for i, vid in enumerate(unique_videos)}\n",
    "idx2video = {i: vid for vid, i in video2idx.items()}\n",
    "\n",
    "\n",
    "# Build user->video edge index\n",
    "user_col = df_watch['user_id'].map(user2idx).values\n",
    "video_col = df_watch['video_id'].map(video2idx).values\n",
    "edge_index_uv = np.vstack([user_col, video_col])\n",
    "\n",
    "df_social = pd.read_csv('KuaiRec/social_network.csv')\n",
    "df_social['friend_list'] = df_social['friend_list'].apply(lambda x: x.strip('[]').split(','))\n",
    "df_social = df_social.explode('friend_list').dropna()\n",
    "df_social['friend_list'] = df_social['friend_list'].astype(int)\n",
    "\n",
    "# Build user-user edge index\n",
    "userA = df_social['user_id'].map(user2idx).values\n",
    "userB = df_social['friend_list'].map(user2idx).values\n",
    "\n",
    "edge_index_uu = np.vstack([userA, userB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "# Add user, video node counts\n",
    "data['user'].num_nodes = len(user2idx)\n",
    "data['video'].num_nodes = len(video2idx)\n",
    "# user->video edges\n",
    "data['user', 'watches', 'video'].edge_index = torch.tensor(edge_index_uv, dtype=torch.long)\n",
    "# Include reverse edges too:\n",
    "data['video', 'watched_by', 'user'].edge_index = torch.tensor(np.flip(edge_index_uv, axis=0).copy(order='C'), dtype=torch.long)\n",
    "data['user', 'follows', 'user'].edge_index = torch.tensor(edge_index_uu, dtype=torch.long)\n",
    "w2v_dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a zero tensor for all video embeddings & update them with W2V trained ones\n",
    "model_vocab = list(model.wv.index_to_key)\n",
    "video_emb = np.zeros((len(video2idx), w2v_dim), dtype=np.float32)\n",
    "for vid, idx in video2idx.items():\n",
    "    if str(vid) in model_vocab:\n",
    "        video_emb[idx] = model.wv[str(vid)]\n",
    "\n",
    "# Set data video embeddings to the W2V embeddings\n",
    "data['video'].x = torch.tensor(video_emb, dtype=torch.float32)\n",
    "\n",
    "num_users = len(user2idx)\n",
    "\n",
    "user_to_videos = defaultdict(list)\n",
    "for _, row in df_watch.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    video_id_str = str(row['video_id'])  # Convert to string for Word2Vec keys\n",
    "    user_to_videos[user_id].append(video_id_str)\n",
    "\n",
    "# Initialize user embeddings to their average video embeddings from training watch history\n",
    "user_emb = torch.zeros((num_users, w2v_dim), dtype=torch.float32)\n",
    "for uid, u_idx in user2idx.items():\n",
    "    vids_watched = user_to_videos[uid]\n",
    "    if not vids_watched:\n",
    "        continue  # user_emb remains zero if no videos\n",
    "\n",
    "    sum_vec = np.zeros(w2v_dim, dtype=np.float32)\n",
    "    count = 0\n",
    "    for vid_str in vids_watched:\n",
    "        if vid_str in model.wv:  # If the video ID is in the W2V vocab\n",
    "            sum_vec += model.wv[vid_str]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        user_emb[u_idx] = torch.tensor(sum_vec / count, dtype=torch.float32)\n",
    "\n",
    "# Set data user embeddings to the user's average W2V video embeddings\n",
    "data['user'].x = user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "meta_path_uuv = [\n",
    "    ('user', 'follows', 'user'),\n",
    "    ('user', 'watches', 'video'),\n",
    "    ('video', 'watched_by', 'user')\n",
    "]\n",
    "chosen_meta_path = meta_path_uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_video_pairs(video_emb, pairs, video2idx, idx2video, k=20):\n",
    "    \"\"\"\n",
    "    video_emb: np.array of shape [num_videos, embedding_dim].\n",
    "    pairs: list of (prev_video_id, next_video_id) as integers or strings.\n",
    "    video2idx: dict mapping video_id -> index in video_emb\n",
    "    idx2video: dict mapping index -> video_id\n",
    "    k: number of neighbors to consider.\n",
    "    \"\"\"\n",
    "    neigh = NearestNeighbors(n_neighbors=k+1, metric='euclidean')  # or 'cosine'\n",
    "    neigh.fit(video_emb)\n",
    "\n",
    "    hrk_score = 0.0\n",
    "    ndcg_score = 0.0\n",
    "    total_eval = 0\n",
    "\n",
    "    for (prev_vid, next_vid) in tqdm(pairs):\n",
    "        # Convert to string if needed, or keep as int if consistent in video2idx\n",
    "        if prev_vid not in video2idx or next_vid not in video2idx:\n",
    "            continue\n",
    "        prev_idx = video2idx[prev_vid]\n",
    "        emb_0 = video_emb[prev_idx].reshape(1, -1)\n",
    "\n",
    "        # Retrieve top (k+1) neighbors, ignoring the first if it is the same item\n",
    "        distances, neighbors_idx = neigh.kneighbors(emb_0, n_neighbors=k+1)\n",
    "        neighbors_idx = neighbors_idx.flatten()[1:]  # skip the \"self\" neighbor\n",
    "\n",
    "        # Convert indices back to video_ids\n",
    "        neighbors_vids = [idx2video[idx] for idx in neighbors_idx]\n",
    "\n",
    "        # Hit Rate @ k\n",
    "        if next_vid in neighbors_vids:\n",
    "            hrk_score += 1.0 / k\n",
    "            # NDCG @ k\n",
    "            rank = neighbors_vids.index(next_vid)  # 0-based\n",
    "            ndcg_score += 1.0 / np.log2(rank + 2)  # rank+1 => position, +1 for 1-based\n",
    "\n",
    "        total_eval += 1\n",
    "\n",
    "    if total_eval == 0:\n",
    "        return {'HR@%i' % k: 0.0, 'NDCG@%i' % k: 0.0}\n",
    "\n",
    "    hrk_score /= total_eval\n",
    "    ndcg_score /= total_eval\n",
    "    return {'HR@%i' % k: 1000*hrk_score, 'NDCG@%i' % k: ndcg_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(model_metapath):\n",
    "    \"\"\"Extract user and video embeddings from the trained model.\"\"\"\n",
    "    model_metapath.eval()\n",
    "    with torch.no_grad():\n",
    "        user_emb = model_metapath('user').cpu().numpy()\n",
    "        video_emb = model_metapath('video').cpu().numpy()\n",
    "    return user_emb, video_emb\n",
    "\n",
    "def run_metapath2vec_training_and_eval(data, meta_path, hp, val_pairs, video2idx, idx2video):\n",
    "    \"\"\"\n",
    "    Trains MetaPath2Vec given hyperparams (hp) and returns best performance on val set.\n",
    "    hp is a dict with keys:\n",
    "      {\n",
    "        'embedding_dim', 'walk_length', 'context_size',\n",
    "        'walks_per_node', 'num_negative_samples', 'epochs', 'lr'\n",
    "      }\n",
    "    Returns: best_metrics, best_epoch\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 1) Build the model\n",
    "    model_metapath = MetaPath2Vec(\n",
    "        data.edge_index_dict,\n",
    "        embedding_dim=hp['embedding_dim'],\n",
    "        metapath=meta_path,\n",
    "        walk_length=hp['walk_length'],\n",
    "        context_size=hp['context_size'],\n",
    "        walks_per_node=hp['walks_per_node'],\n",
    "        num_negative_samples=hp['num_negative_samples'],\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Overwrite \"video\" embeddings from data['video'].x if you have W2V init\n",
    "    with torch.no_grad():\n",
    "        model_metapath('video').data.copy_(data['video'].x.to(device))\n",
    "        model_metapath('user').data.copy_(data['user'].x.to(device))\n",
    "    \n",
    "    loader = model_metapath.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(model_metapath.parameters(), lr=hp['lr'])\n",
    "    \n",
    "    # 2) Training with early stopping (two consecutive hits@20 decreases)\n",
    "    best_metrics = {'HR@20': 0.0, 'NDCG@20': 0.0}\n",
    "    best_epoch = 0\n",
    "    consecutive_decreases = 0\n",
    "    \n",
    "    def train_one_epoch(epoch):\n",
    "        model_metapath.train()\n",
    "        total_loss = 0\n",
    "        for step, (pos_rw, neg_rw) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            pos_rw, neg_rw = pos_rw.to(device), neg_rw.to(device)\n",
    "            loss = model_metapath.loss(pos_rw, neg_rw)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss\n",
    "    \n",
    "    for epoch in range(1, hp['epochs'] + 1):\n",
    "        epoch_loss = train_one_epoch(epoch)\n",
    "        \n",
    "        # Evaluate on val set\n",
    "        user_emb, video_emb = get_embeddings(model_metapath)  # updated: pass model\n",
    "        val_metrics = evaluate_video_pairs(video_emb, val_pairs, video2idx, idx2video, k=20)\n",
    "        hr20 = val_metrics['HR@20']\n",
    "                \n",
    "        # Check if this is best\n",
    "        if hr20 > best_metrics['HR@20']:\n",
    "            best_metrics = val_metrics\n",
    "            best_epoch = epoch\n",
    "            consecutive_decreases = 0\n",
    "        else:\n",
    "            consecutive_decreases += 1\n",
    "        \n",
    "        # Stop if 2 consecutive decreases in HR@20\n",
    "        if consecutive_decreases >= 2:\n",
    "            break\n",
    "    \n",
    "    return best_metrics, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def hyperparam_search(data, val_pairs, video2idx, idx2video):\n",
    "    walk_lengths = [6, 10, 20]         \n",
    "    context_sizes = [2, 5, 10]            \n",
    "    walks_per_nodes = [2, 5, 10]          # how many random walks each node starts\n",
    "    neg_samples = [2, 5, 10]             # negative sampling\n",
    "    lrs = [0.01]                       # learning rates\n",
    "    epochs = 15                       # max epochs for each\n",
    "    meta_paths = [\n",
    "        [('user', 'follows', 'user'), ('user', 'watches', 'video'), ('video', 'watched_by', 'user')]\n",
    "    ]\n",
    "    \n",
    "    best_config = None\n",
    "    best_metrics = {'HR@20': 0.0, 'NDCG@20': 0.0}\n",
    "    \n",
    "    for meta_path in meta_paths:\n",
    "        for wlen in walk_lengths:\n",
    "            for csize in context_sizes:\n",
    "                if wlen + 1 < csize:\n",
    "                    continue\n",
    "                for wpn in walks_per_nodes:\n",
    "                    for neg in neg_samples:\n",
    "                        for lr in lrs:\n",
    "                            hp = {\n",
    "                                'embedding_dim': w2v_dim,\n",
    "                                'walk_length': wlen,\n",
    "                                'context_size': csize,\n",
    "                                'walks_per_node': wpn,\n",
    "                                'num_negative_samples': neg,\n",
    "                                'epochs': epochs,\n",
    "                                'lr': lr\n",
    "                            }\n",
    "\n",
    "                            metrics, best_epoch = run_metapath2vec_training_and_eval(\n",
    "                                data, meta_path, hp, val_pairs, video2idx, idx2video\n",
    "                            )\n",
    "                            # Check if better\n",
    "                            if metrics['HR@20'] > best_metrics['HR@20']:\n",
    "                                best_metrics = metrics\n",
    "                                best_config = (meta_path, hp, best_epoch)\n",
    "    \n",
    "    return best_config, best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "best_config, best_val_metrics = hyperparam_search(data, val, video2idx, idx2video)\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"Best val metrics:\", best_val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Best metapath2vec embedding performance on test set\n",
    "hp = {\n",
    "    'embedding_dim': w2v_dim,\n",
    "    'walk_length': 6,\n",
    "    'context_size': 6,\n",
    "    'walks_per_node': 2,\n",
    "    'num_negative_samples': 8,\n",
    "    'epochs': 10,\n",
    "    'lr': 0.01\n",
    "}\n",
    "\n",
    "metrics, best_epoch = run_metapath2vec_training_and_eval(\n",
    "    data, meta_path_uuv, hp, val, video2idx, idx2video\n",
    ")\n",
    "print(best_epoch, metrics)\n",
    "metrics, best_epoch = run_metapath2vec_training_and_eval(\n",
    "    data, meta_path_uuv, hp, test, video2idx, idx2video\n",
    ")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Frozen-Metapath2Vec\n",
    "Here we attempt to see if we can achieve similar performance while freezing the video embeddings (0 gradient). This would allow only the user embeddings to adapt to the already pretrained W2V video embeddings, and would allow use to not have to store two sets of track/video embeddings (for non-personalized and personalized recommendation). But as we can see below, this fails miserably and the performance almost goes to 0, indicating that changes in the video embeddings are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.index import index2ptr\n",
    "from torch_geometric.typing import EdgeType, NodeType, OptTensor\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "def sample(\n",
    "    rowptr: Tensor,\n",
    "    col: Tensor,\n",
    "    rowcount: Tensor,\n",
    "    subset: Tensor,\n",
    "    num_neighbors: int,\n",
    "    dummy_idx: int\n",
    ") -> Tensor:\n",
    "    r\"\"\"Samples a single neighbor for each node in :obj:`subset` based on\n",
    "    :obj:`rowptr` and :obj:`col`. Returns :obj:`dummy_idx` for nodes\n",
    "    without neighbors (or out-of-bounds).\"\"\"\n",
    "\n",
    "    # Mask out-of-bounds or dummy nodes:\n",
    "    mask = subset >= dummy_idx\n",
    "    subset = subset.clamp(min=0, max=rowptr.size(0) - 2)\n",
    "\n",
    "    # How many neighbors does each row have?\n",
    "    count = rowcount[subset]  # [|subset|]\n",
    "\n",
    "    # Sample indices:\n",
    "    rand = torch.rand((subset.size(0), num_neighbors), device=subset.device)\n",
    "    rand *= count.to(rand.dtype).view(-1, 1)  # scale random floats by neighbor count\n",
    "    rand = rand.to(torch.long) + rowptr[subset].view(-1, 1)\n",
    "    rand = rand.clamp(max=col.numel() - 1)  # safe clamp in case of tiny col\n",
    "\n",
    "    out = col[rand] if col.numel() > 0 else rand\n",
    "    # For isolated nodes (count=0) or originally out-of-bounds => dummy_idx\n",
    "    out[mask | (count == 0)] = dummy_idx\n",
    "    return out\n",
    "\n",
    "\n",
    "class FrozenMetaPath2Vec(nn.Module):\n",
    "    r\"\"\"\n",
    "    A custom MetaPath2Vec model that learns separate embeddings for \"user\" and\n",
    "    \"video\" node types, following the paper:\n",
    "\n",
    "    \"metapath2vec: Scalable Representation Learning for Heterogeneous Networks\"\n",
    "    (Dong, Chawla, & Swami, KDD'17).\n",
    "\n",
    "    This version:\n",
    "      • Uses random walks based on a specified :obj:`metapath`\n",
    "      • Samples positive and negative context windows\n",
    "      • Stores separate `Embedding` layers for \"user\" and \"video\"\n",
    "        so you can freeze the video embeddings and train only the user embeddings\n",
    "\n",
    "    Args:\n",
    "        edge_index_dict (Dict[EdgeType, Tensor]): Dictionary holding edge\n",
    "            indices for each :obj:`(src_type, rel_type, dst_type)`.\n",
    "        user_count (int): Number of user nodes.\n",
    "        video_count (int): Number of video nodes.\n",
    "        embedding_dim (int): Embedding dimension size.\n",
    "        metapath (List[EdgeType]): The metapath described as a list of\n",
    "            `(src_node_type, rel_type, dst_node_type)`.\n",
    "        walk_length (int): The length of each random walk.\n",
    "        context_size (int): The actual context size for positive samples.\n",
    "        walks_per_node (int, optional): Number of random walks per node.\n",
    "            (default: :obj:`1`)\n",
    "        num_negative_samples (int, optional): Number of negative samples for\n",
    "            each positive example. (default: :obj:`1`)\n",
    "        sparse (bool, optional): If set to True, use sparse gradients for\n",
    "            embedding parameters. (default: :obj:`False`)\n",
    "\n",
    "    **How to freeze video embeddings**:\n",
    "        1. Copy in your pre-trained vectors:\n",
    "           ```\n",
    "           with torch.no_grad():\n",
    "               model.video_embedding.weight.copy_(pretrained_video_emb)\n",
    "           ```\n",
    "        2. Set\n",
    "           ```\n",
    "           model.video_embedding.weight.requires_grad = False\n",
    "           ```\n",
    "        3. Construct your optimizer only over remaining parameters:\n",
    "           ```\n",
    "           optimizer = torch.optim.SparseAdam(\n",
    "               filter(lambda p: p.requires_grad, model.parameters()),\n",
    "               lr=0.01\n",
    "           )\n",
    "           ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_index_dict: Dict[Tuple[str, str, str], Tensor],\n",
    "        user_count: int,\n",
    "        video_count: int,\n",
    "        embedding_dim: int,\n",
    "        metapath: List[EdgeType],\n",
    "        walk_length: int,\n",
    "        context_size: int,\n",
    "        walks_per_node: int = 1,\n",
    "        num_negative_samples: int = 1,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_count = user_count\n",
    "        self.video_count = video_count\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Two distinct embeddings:\n",
    "        #   user_embedding: for nodes in [0 .. user_count-1]\n",
    "        #   video_embedding: for nodes in [0 .. video_count-1]\n",
    "        self.user_embedding = nn.Embedding(user_count, embedding_dim, sparse=sparse)\n",
    "        self.video_embedding = nn.Embedding(video_count, embedding_dim, sparse=sparse)\n",
    "\n",
    "        # Metapath hyperparams:\n",
    "        self.metapath = metapath\n",
    "        self.walk_length = walk_length\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "        # Preprocess adjacency for each edge type:\n",
    "        # rowptr_dict[(src, rel, dst)], col_dict[(src, rel, dst)], etc.\n",
    "        self.rowptr_dict = {}\n",
    "        self.col_dict = {}\n",
    "        self.rowcount_dict = {}\n",
    "\n",
    "        # We assume exactly two node types: \"user\" and \"video\".\n",
    "        # Each edge_index is for either user->video or video->user or user->user, etc.\n",
    "        # We'll store them carefully so random walks can jump between them.\n",
    "        for keys, edge_index in edge_index_dict.items():\n",
    "            src_type, _, dst_type = keys\n",
    "            # Figure out how many nodes on source/dest side:\n",
    "            if src_type == \"user\":\n",
    "                src_size = user_count\n",
    "            elif src_type == \"video\":\n",
    "                src_size = video_count\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown src_type: {src_type}\")\n",
    "\n",
    "            if dst_type == \"user\":\n",
    "                dst_size = user_count\n",
    "            elif dst_type == \"video\":\n",
    "                dst_size = video_count\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown dst_type: {dst_type}\")\n",
    "\n",
    "            # Sort edges and get rowptr, col for sampling:\n",
    "            row, col = sort_edge_index(edge_index, num_nodes=max(src_size, dst_size))\n",
    "            rowptr = index2ptr(row, size=src_size)\n",
    "            self.rowptr_dict[keys] = rowptr\n",
    "            self.col_dict[keys] = col\n",
    "            self.rowcount_dict[keys] = rowptr[1:] - rowptr[:-1]\n",
    "\n",
    "        # Sanity check on metapath continuity:\n",
    "        for edge_type1, edge_type2 in zip(metapath[:-1], metapath[1:]):\n",
    "            # E.g., (user, \"rel1\", video), (video, \"rel2\", user) => OK\n",
    "            if edge_type1[-1] != edge_type2[0]:\n",
    "                raise ValueError(\n",
    "                    \"Invalid metapath: destination node type of one edge \"\n",
    "                    \"does not match source node type of next edge.\"\n",
    "                )\n",
    "\n",
    "        # The paper requires: walk_length + 1 >= context_size\n",
    "        assert walk_length + 1 >= context_size, \\\n",
    "            \"'walk_length + 1' must be >= 'context_size'.\"\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        r\"\"\"Resets user and video embedding parameters.\"\"\"\n",
    "        # If you do NOT want to reset the video embedding (e.g. it's pre-trained),\n",
    "        # comment out the next line:\n",
    "        self.user_embedding.reset_parameters()\n",
    "        self.video_embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, node_type: str, batch: OptTensor = None) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Returns the embedding for a given node type (`\"user\"` or `\"video\"`).\n",
    "        If batch is None, returns all embeddings of that type.\n",
    "        \"\"\"\n",
    "        if node_type == \"user\":\n",
    "            emb = self.user_embedding.weight\n",
    "        elif node_type == \"video\":\n",
    "            emb = self.video_embedding.weight\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported node type: {node_type}\")\n",
    "\n",
    "        if batch is None:\n",
    "            return emb\n",
    "        else:\n",
    "            return emb.index_select(0, batch)\n",
    "\n",
    "    def loader(self, node_type: str, **kwargs):\n",
    "        r\"\"\"\n",
    "        Returns a DataLoader over the nodes of a particular type, e.g. \"user\".\n",
    "        We'll create random walks starting from each node of that type.\n",
    "        The DataLoader yields `(pos_rw, neg_rw)` pairs.\n",
    "        \"\"\"\n",
    "        if node_type not in (\"user\", \"video\"):\n",
    "            raise ValueError(\"'loader(node_type=...)' must be 'user' or 'video'.\")\n",
    "\n",
    "        if node_type == \"user\":\n",
    "            num_nodes = self.user_count\n",
    "        else:  # video\n",
    "            num_nodes = self.video_count\n",
    "\n",
    "        return DataLoader(range(num_nodes), collate_fn=self._sample, **kwargs)\n",
    "\n",
    "    def _sample(self, batch_list: List[int]) -> Tuple[Tensor, Tensor]:\n",
    "        r\"\"\"Given a list of node IDs of a certain type, sample positive and negative\n",
    "        random walks. This is passed to the DataLoader as `collate_fn`.\"\"\"\n",
    "        batch = torch.tensor(batch_list, dtype=torch.long)\n",
    "        return self._pos_sample(batch), self._neg_sample(batch)\n",
    "\n",
    "    def _pos_sample(self, batch: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Samples positive random walks along the metapath, starting from `batch`.\n",
    "        We'll produce multiple walks if `walks_per_node > 1`.\n",
    "        \"\"\"\n",
    "        batch = batch.repeat(self.walks_per_node)\n",
    "        rws = [batch]  # list of [nodes at step0, nodes at step1, ...]\n",
    "\n",
    "        # Follow the edges in `self.metapath` repeatedly for `walk_length` steps:\n",
    "        for i in range(self.walk_length):\n",
    "            edge_type = self.metapath[i % len(self.metapath)]\n",
    "            rowptr = self.rowptr_dict[edge_type]\n",
    "            col = self.col_dict[edge_type]\n",
    "            rowcount = self.rowcount_dict[edge_type]\n",
    "\n",
    "            # Sample next node:\n",
    "            batch = sample(\n",
    "                rowptr, col, rowcount, batch,\n",
    "                num_neighbors=1,\n",
    "                dummy_idx=max(self.user_count, self.video_count)  # safe dummy\n",
    "            ).view(-1)\n",
    "            rws.append(batch)\n",
    "\n",
    "        # Now we have a list of Tensors (walk_length+1 steps). Combine them:\n",
    "        # For context-windowing, we create overlapping windows of size `context_size`.\n",
    "        pos_rw = self._build_context_windows(rws)\n",
    "        return pos_rw\n",
    "\n",
    "    def _neg_sample(self, batch: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Negative sampling: pick random nodes at each step (matching the\n",
    "        node type of the current metapath), ignoring adjacency.\n",
    "        \"\"\"\n",
    "        batch = batch.repeat(self.walks_per_node * self.num_negative_samples)\n",
    "        rws = [batch]\n",
    "\n",
    "        for i in range(self.walk_length):\n",
    "            _, _, dst_type = self.metapath[i % len(self.metapath)]\n",
    "            if dst_type == \"user\":\n",
    "                num_nodes = self.user_count\n",
    "            elif dst_type == \"video\":\n",
    "                num_nodes = self.video_count\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dst_type: {dst_type}\")\n",
    "\n",
    "            batch = torch.randint(0, num_nodes, (batch.size(0),), dtype=torch.long)\n",
    "            rws.append(batch)\n",
    "\n",
    "        neg_rw = self._build_context_windows(rws)\n",
    "        return neg_rw\n",
    "\n",
    "    def _build_context_windows(self, rws: List[Tensor]) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Takes a list of Tensors, each shape [num_walks], representing the node\n",
    "        IDs at each step in the walk. Then builds context windows of size\n",
    "        `self.context_size`.\n",
    "        \"\"\"\n",
    "        # Stack steps into shape [num_walks, walk_length+1]\n",
    "        rw_stack = torch.stack(rws, dim=-1)\n",
    "        total_steps = len(rws)  # walk_length + 1\n",
    "\n",
    "        # We'll make (total_steps - context_size + 1) windows per walk.\n",
    "        num_walks_per_rw = total_steps - self.context_size + 1\n",
    "        out = []\n",
    "        for j in range(num_walks_per_rw):\n",
    "            # shape: [num_walks, context_size]\n",
    "            window = rw_stack[:, j : j + self.context_size]\n",
    "            out.append(window)\n",
    "        # shape: [num_windows * num_walks, context_size]\n",
    "        return torch.cat(out, dim=0)\n",
    "\n",
    "    def loss(self, pos_rw: Tensor, neg_rw: Tensor) -> Tensor:\n",
    "        r\"\"\"Computes the negative-sampling loss for the given positive and negative\n",
    "        random walks.\"\"\"\n",
    "        # Positive samples:\n",
    "        start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "        # Embed them:\n",
    "        h_start = self._embed_nodes(start).view(pos_rw.size(0), 1, self.embedding_dim)\n",
    "        h_rest = self._embed_nodes(rest.view(-1)).view(pos_rw.size(0), -1, self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        # Negative samples:\n",
    "        start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "        h_start = self._embed_nodes(start).view(neg_rw.size(0), 1, self.embedding_dim)\n",
    "        h_rest = self._embed_nodes(rest.view(-1)).view(neg_rw.size(0), -1, self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def _embed_nodes(self, nodes: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        Looks up embeddings for a batch of node indices, which may contain both\n",
    "        user and video IDs. We must figure out which are users vs. videos.\n",
    "        For simplicity, we assume node IDs < user_count => \"user\",\n",
    "        else => \"video\" (subtract user_count).\n",
    "        \"\"\"\n",
    "        device = nodes.device\n",
    "        emb = torch.zeros((nodes.size(0), self.embedding_dim), device=device)\n",
    "\n",
    "        user_mask = (nodes < self.user_count)\n",
    "        if user_mask.any():\n",
    "            user_idx = nodes[user_mask]\n",
    "            emb[user_mask] = self.user_embedding(user_idx)\n",
    "\n",
    "        video_mask = ~user_mask\n",
    "        if video_mask.any():\n",
    "            video_idx = nodes[video_mask] - self.user_count\n",
    "            emb[video_mask] = self.video_embedding(video_idx.clamp(min=0))\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def test(\n",
    "        self,\n",
    "        train_z: Tensor,\n",
    "        train_y: Tensor,\n",
    "        test_z: Tensor,\n",
    "        test_y: Tensor,\n",
    "        solver: str = \"lbfgs\",\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> float:\n",
    "        r\"\"\"\n",
    "        Evaluates latent space quality via a logistic regression downstream task.\n",
    "        (Kept for parity with PyG’s MetaPath2Vec.)\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        clf = LogisticRegression(solver=solver, *args, **kwargs).fit(\n",
    "            train_z.detach().cpu().numpy(),\n",
    "            train_y.detach().cpu().numpy()\n",
    "        )\n",
    "        return clf.score(test_z.detach().cpu().numpy(), test_y.detach().cpu().numpy())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f\"{self.__class__.__name__}(\\n\"\n",
    "                f\"  user_count={self.user_count}, video_count={self.video_count}, \"\n",
    "                f\"embedding_dim={self.embedding_dim}\\n)\")\n",
    "\n",
    "\n",
    "model_metapath=FrozenMetaPath2Vec(\n",
    "    data.edge_index_dict,\n",
    "    user_count=data['user']['num_nodes'], video_count=data['video']['num_nodes'],\n",
    "    embedding_dim=w2v_dim,\n",
    "    metapath=meta_path_uuv,\n",
    "    walk_length=6, context_size=3,\n",
    "    walks_per_node=2, num_negative_samples=5,\n",
    "    sparse=True\n",
    ").to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_metapath.video_embedding.weight.copy_(data['video'].x.to('cuda'))\n",
    "    model_metapath.user_embedding.weight.copy_(data['user'].x.to('cuda'))\n",
    "# Freeze them:\n",
    "model_metapath.video_embedding.weight.requires_grad = False\n",
    "\n",
    "original_video_w2v_cpu = data['video'].x.clone().cpu()\n",
    "\n",
    "assert torch.allclose(\n",
    "    model_metapath.video_embedding.weight.detach().cpu(),\n",
    "    original_video_w2v_cpu,\n",
    "    atol=1e-6\n",
    "), \"Mismatch even before training!\"\n",
    "\n",
    "optimizer = torch.optim.SparseAdam(\n",
    "    filter(lambda p: p.requires_grad, model_metapath.parameters()), lr=0.01\n",
    ")\n",
    "loader = model_metapath.loader(node_type='user', batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "def train_one_epoch():\n",
    "    model_metapath.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        pos_rw, neg_rw = pos_rw.to('cuda'), neg_rw.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        loss = model_metapath.loss(pos_rw, neg_rw)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    epoch_loss = train_one_epoch()\n",
    "    print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on validation:\n",
    "    model_metapath.eval()\n",
    "    user_emb = model_metapath('user').cpu().detach().numpy()\n",
    "    video_emb = model_metapath('video').cpu().detach().numpy()\n",
    "    \n",
    "    val_metrics = evaluate_user_reco(\n",
    "        user_emb=user_emb,\n",
    "        video_emb=video_emb,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        user2idx=user2idx,\n",
    "        video2idx=video2idx,\n",
    "        idx2video=idx2video,\n",
    "        is_validation=True,   # i.e., we're evaluating on val_data\n",
    "        top_k=100\n",
    "    )\n",
    "    print(\"Val metrics:\", val_metrics)\n",
    "    \n",
    "    current_video_emb = model_metapath.video_embedding.weight.detach().cpu()\n",
    "    if torch.allclose(current_video_emb, original_video_w2v_cpu, atol=1e-6):\n",
    "        print(\"Video embeddings are unchanged after epoch\", epoch)\n",
    "    else:\n",
    "        print(\"WARNING: Video embeddings changed after epoch\", epoch)\n",
    "\n",
    "final_video_emb = model_metapath.video_embedding.weight.detach().cpu()\n",
    "assert torch.allclose(final_video_emb, original_video_w2v_cpu, atol=1e-6), (\n",
    "    \"Video embeddings diverged from the original W2V!\"\n",
    ")\n",
    "print(\"Training finished; video embeddings remained fixed.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
