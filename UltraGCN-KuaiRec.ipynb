{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d21fae-735b-4391-9626-adb42c92d3b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## UltraGCN KuaiRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e600a9c-e4f8-449a-aac1-7f0b0ed45226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 14666.1238\n",
      "Test Metrics: {'Recall@1': 0.006891385514725286, 'Recall@2': 0.011873115186467737, 'Recall@3': 0.015428774474037383, 'Recall@5': 0.022695493450765266, 'Recall@10': 0.03516822008790371, 'Recall@20': 0.05337984265872318, 'Recall@30': 0.06915117598449312, 'Recall@40': 0.0825899097208964, 'Recall@50': 0.09558622840372709, 'Recall@60': 0.105164251826902, 'Recall@70': 0.11464980536034802, 'Recall@80': 0.12200531416562585, 'Recall@90': 0.129472217602437, 'Recall@100': 0.13701526784123028, 'NDCG@1': 0.0963855421686747, 'NDCG@2': 0.08816046476468019, 'NDCG@3': 0.08104624108585463, 'NDCG@5': 0.07487380822636895, 'NDCG@10': 0.0688693441717888, 'NDCG@20': 0.06811990350473553, 'NDCG@30': 0.07073308866113709, 'NDCG@40': 0.0734943292876321, 'NDCG@50': 0.07715753626376758, 'NDCG@60': 0.07986772067833171, 'NDCG@70': 0.08292367190351789, 'NDCG@80': 0.08545078647252426, 'NDCG@90': 0.08809902397118079, 'NDCG@100': 0.09072497359941749, 'Coverage@1': 0.010439970171513796, 'Coverage@2': 0.017058165548098435, 'Coverage@3': 0.022557792692020878, 'Coverage@5': 0.031785980611483966, 'Coverage@10': 0.05080164056674124, 'Coverage@20': 0.08202833706189411, 'Coverage@30': 0.10710290827740492, 'Coverage@40': 0.1303131991051454, 'Coverage@50': 0.14988814317673377, 'Coverage@60': 0.16778523489932887, 'Coverage@70': 0.18363161819537657, 'Coverage@80': 0.1994780014914243, 'Coverage@90': 0.2140193885160328, 'Coverage@100': 0.2273489932885906}\n",
      "Epoch 2, Loss: 8252.7771\n",
      "Test Metrics: {'Recall@1': 0.014787096596869742, 'Recall@2': 0.024075833970419295, 'Recall@3': 0.030899501463351346, 'Recall@5': 0.041154374970321976, 'Recall@10': 0.06362907359366071, 'Recall@20': 0.09638106177265052, 'Recall@30': 0.12155188384909632, 'Recall@40': 0.1460844101911218, 'Recall@50': 0.16406683026515453, 'Recall@60': 0.18032576462335054, 'Recall@70': 0.19477091623474338, 'Recall@80': 0.2065730479579225, 'Recall@90': 0.22062801412358898, 'Recall@100': 0.23121644818143047, 'NDCG@1': 0.1771793054571226, 'NDCG@2': 0.1549715964663376, 'NDCG@3': 0.14177486190706826, 'NDCG@5': 0.12503019067468396, 'NDCG@10': 0.11322067897096032, 'NDCG@20': 0.11434845958802534, 'NDCG@30': 0.11942270774771258, 'NDCG@40': 0.12576479512420968, 'NDCG@50': 0.13101458294230092, 'NDCG@60': 0.13616115029900333, 'NDCG@70': 0.1408908989923111, 'NDCG@80': 0.14514501203340086, 'NDCG@90': 0.14970062321124994, 'NDCG@100': 0.1535132140085654, 'Coverage@1': 0.0070842654735272185, 'Coverage@2': 0.011278896346010439, 'Coverage@3': 0.015007457121551082, 'Coverage@5': 0.02097315436241611, 'Coverage@10': 0.032997762863534674, 'Coverage@20': 0.054436987322893364, 'Coverage@30': 0.07280014914243102, 'Coverage@40': 0.08771439224459358, 'Coverage@50': 0.10262863534675615, 'Coverage@60': 0.11623788217747949, 'Coverage@70': 0.12863534675615212, 'Coverage@80': 0.14336316181953765, 'Coverage@90': 0.15678598061148397, 'Coverage@100': 0.1679716629381059}\n",
      "Epoch 3, Loss: 6859.7820\n",
      "Test Metrics: {'Recall@1': 0.01596043584200058, 'Recall@2': 0.026105316322979303, 'Recall@3': 0.034256167428746494, 'Recall@5': 0.04802521267791222, 'Recall@10': 0.07119325736349727, 'Recall@20': 0.1059558123828138, 'Recall@30': 0.13361341822169936, 'Recall@40': 0.15480058895307675, 'Recall@50': 0.17293440305003624, 'Recall@60': 0.18826513338255324, 'Recall@70': 0.20110752880124524, 'Recall@80': 0.21511620632987546, 'Recall@90': 0.22525765542398896, 'Recall@100': 0.23495250003566795, 'NDCG@1': 0.17647058823529413, 'NDCG@2': 0.16056877192090466, 'NDCG@3': 0.1448303380784911, 'NDCG@5': 0.12976672322087485, 'NDCG@10': 0.11845560463567927, 'NDCG@20': 0.11901025045862475, 'NDCG@30': 0.12569910361446307, 'NDCG@40': 0.13104585628569498, 'NDCG@50': 0.13644487294681448, 'NDCG@60': 0.14121001869952002, 'NDCG@70': 0.14532840965157567, 'NDCG@80': 0.15004641652814976, 'NDCG@90': 0.15367368168342116, 'NDCG@100': 0.15731170630273597, 'Coverage@1': 0.007550335570469799, 'Coverage@2': 0.011185682326621925, 'Coverage@3': 0.014168530947054437, 'Coverage@5': 0.01929530201342282, 'Coverage@10': 0.029269202087994033, 'Coverage@20': 0.04679343773303505, 'Coverage@30': 0.06086875466070097, 'Coverage@40': 0.07307979120059657, 'Coverage@50': 0.08435868754660701, 'Coverage@60': 0.09573079791200596, 'Coverage@70': 0.1058911260253542, 'Coverage@80': 0.1185682326621924, 'Coverage@90': 0.12742356450410142, 'Coverage@100': 0.1372110365398956}\n",
      "Epoch 4, Loss: 6412.5478\n",
      "Test Metrics: {'Recall@1': 0.016833439999574883, 'Recall@2': 0.027413131632575992, 'Recall@3': 0.03757483597006482, 'Recall@5': 0.051058576045371555, 'Recall@10': 0.07036364082432234, 'Recall@20': 0.10129601697400659, 'Recall@30': 0.12770910145478295, 'Recall@40': 0.14499729921248175, 'Recall@50': 0.1645560523001914, 'Recall@60': 0.17890072325221046, 'Recall@70': 0.19332985113826834, 'Recall@80': 0.20588982557229546, 'Recall@90': 0.21809466607462716, 'Recall@100': 0.2285583371377337, 'NDCG@1': 0.1856839121190645, 'NDCG@2': 0.16347620312827948, 'NDCG@3': 0.15186471737903698, 'NDCG@5': 0.13383609693437984, 'NDCG@10': 0.11630887471957929, 'NDCG@20': 0.11558457606568047, 'NDCG@30': 0.12117300178367839, 'NDCG@40': 0.12569801567351996, 'NDCG@50': 0.1318340977332783, 'NDCG@60': 0.13655692727069513, 'NDCG@70': 0.14146817326694824, 'NDCG@80': 0.14582845296885882, 'NDCG@90': 0.14983732328446187, 'NDCG@100': 0.15351932856908593, 'Coverage@1': 0.007923191648023863, 'Coverage@2': 0.010626398210290829, 'Coverage@3': 0.013329604772557792, 'Coverage@5': 0.01780387770320656, 'Coverage@10': 0.0273117076808352, 'Coverage@20': 0.04148023862788963, 'Coverage@30': 0.053131991051454136, 'Coverage@40': 0.06431767337807606, 'Coverage@50': 0.0732662192393736, 'Coverage@60': 0.08454511558538404, 'Coverage@70': 0.09507829977628636, 'Coverage@80': 0.10477255779269203, 'Coverage@90': 0.1127889634601044, 'Coverage@100': 0.12173750932140194}\n",
      "Epoch 5, Loss: 6109.4821\n",
      "Test Metrics: {'Recall@1': 0.012475374457759652, 'Recall@2': 0.018804946674783624, 'Recall@3': 0.02657430562639035, 'Recall@5': 0.03766373100623896, 'Recall@10': 0.05719251616492988, 'Recall@20': 0.08533165004327552, 'Recall@30': 0.10649901601369811, 'Recall@40': 0.12796049493887765, 'Recall@50': 0.14610234434218583, 'Recall@60': 0.16448301542667146, 'Recall@70': 0.1828160336956401, 'Recall@80': 0.19829085473972, 'Recall@90': 0.21123524355280704, 'Recall@100': 0.22022246906026108, 'NDCG@1': 0.12969525159461376, 'NDCG@2': 0.11077757356542639, 'NDCG@3': 0.10316563178410344, 'NDCG@5': 0.09456359229793736, 'NDCG@10': 0.08619130318543243, 'NDCG@20': 0.08914449444639067, 'NDCG@30': 0.0949595859814413, 'NDCG@40': 0.10139313786984752, 'NDCG@50': 0.10751378502277614, 'NDCG@60': 0.11377182137008202, 'NDCG@70': 0.12018300665764266, 'NDCG@80': 0.12566147052326282, 'NDCG@90': 0.13013267064551295, 'NDCG@100': 0.13338040089599196, 'Coverage@1': 0.007736763609246831, 'Coverage@2': 0.01174496644295302, 'Coverage@3': 0.01435495898583147, 'Coverage@5': 0.01836316181953766, 'Coverage@10': 0.027404921700223715, 'Coverage@20': 0.041200596569724086, 'Coverage@30': 0.052572706935123045, 'Coverage@40': 0.06329231916480238, 'Coverage@50': 0.07522371364653244, 'Coverage@60': 0.08510439970171514, 'Coverage@70': 0.09526472781506339, 'Coverage@80': 0.10449291573452647, 'Coverage@90': 0.11474645786726323, 'Coverage@100': 0.12369500372856078}\n",
      "Epoch 6, Loss: 5878.0202\n",
      "Test Metrics: {'Recall@1': 0.014313556332297398, 'Recall@2': 0.022488203873912455, 'Recall@3': 0.02942155314473705, 'Recall@5': 0.03897785748070155, 'Recall@10': 0.06013428415776458, 'Recall@20': 0.08585278930636314, 'Recall@30': 0.1080346348724632, 'Recall@40': 0.129965452296123, 'Recall@50': 0.14901441317844702, 'Recall@60': 0.16298655798344266, 'Recall@70': 0.1778978203366124, 'Recall@80': 0.1920308631838353, 'Recall@90': 0.20343392566407503, 'Recall@100': 0.21315459427605388, 'NDCG@1': 0.13819985825655565, 'NDCG@2': 0.12037885721456758, 'NDCG@3': 0.1094010066813526, 'NDCG@5': 0.09685603561629164, 'NDCG@10': 0.09007513345875738, 'NDCG@20': 0.09160200742211685, 'NDCG@30': 0.09730349417954662, 'NDCG@40': 0.10424265646019165, 'NDCG@50': 0.11021275518967133, 'NDCG@60': 0.11479341116145997, 'NDCG@70': 0.11995423092016833, 'NDCG@80': 0.12478106522613691, 'NDCG@90': 0.12879304412447684, 'NDCG@100': 0.13233118111312295, 'Coverage@1': 0.008109619686800894, 'Coverage@2': 0.01174496644295302, 'Coverage@3': 0.0145413870246085, 'Coverage@5': 0.018922445935868754, 'Coverage@10': 0.028616703952274423, 'Coverage@20': 0.04203952274422073, 'Coverage@30': 0.055089485458612975, 'Coverage@40': 0.06562266964951528, 'Coverage@50': 0.07643549589858314, 'Coverage@60': 0.08743475018642805, 'Coverage@70': 0.09843400447427293, 'Coverage@80': 0.10859433258762118, 'Coverage@90': 0.11707680835197613, 'Coverage@100': 0.12705070842654737}\n",
      "Early stop at epoch 6, Best Recall@100: 0.2350 at epoch 3\n"
     ]
    }
   ],
   "source": [
    "# With W2v Init\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# Step 1: Precompute W2V Embeddings\n",
    "def w2v_emb(embedding_dim=50):\n",
    "    df_watch = pd.read_csv('KuaiRec/big_matrix.csv')\n",
    "    df_watch = df_watch[df_watch['watch_ratio'] >= 2].sort_values(['user_id', 'time'])\n",
    "\n",
    "    unique_users = df_watch['user_id'].unique()\n",
    "    unique_videos = df_watch['video_id'].unique()\n",
    "\n",
    "    user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
    "    video2idx = {vid: i for i, vid in enumerate(unique_videos)}\n",
    "    \n",
    "    train = pd.read_parquet('KuaiRec/train_sequences.parquet')\n",
    "    sequences = train['video_id'].apply(lambda x: list(map(str, x))).tolist()\n",
    "    model = Word2Vec(\n",
    "        vector_size=embedding_dim,\n",
    "        window=15,\n",
    "        workers=multiprocessing.cpu_count(),\n",
    "        sg=1,\n",
    "        min_count=1,\n",
    "        compute_loss=True,\n",
    "    )\n",
    "    model.build_vocab(sequences)\n",
    "    model.train(\n",
    "        corpus_iterable=sequences,\n",
    "        total_examples=len(sequences),\n",
    "        epochs=75,\n",
    "    )\n",
    "    model_vocab = list(model.wv.index_to_key)\n",
    "    \n",
    "    with open(\"KuaiRec/adj_data.pkl\", \"rb\") as f:\n",
    "        adj_data = pickle.load(f)\n",
    "        \n",
    "    video_emb = np.zeros((adj_data['num_nodes_dict']['video'], embedding_dim), dtype=np.float32)\n",
    "    for vid, idx in video2idx.items():\n",
    "        if str(vid) in model_vocab:\n",
    "            video_emb[idx] = model.wv[str(vid)]\n",
    "\n",
    "    video_emb = torch.tensor(video_emb, dtype=torch.float32)\n",
    "    \n",
    "    num_users = adj_data['num_nodes_dict']['user']\n",
    "    user_to_videos = defaultdict(list)\n",
    "    for _, row in df_watch.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        video_id_str = str(row['video_id'])\n",
    "        user_to_videos[user_id].append(video_id_str)\n",
    "\n",
    "    user_emb = torch.zeros((num_users, embedding_dim), dtype=torch.float32)\n",
    "    for uid, u_idx in user2idx.items():\n",
    "        vids_watched = user_to_videos[uid]\n",
    "        if not vids_watched:\n",
    "            continue\n",
    "        sum_vec = np.zeros(embedding_dim, dtype=np.float32)\n",
    "        count = 0\n",
    "        for vid_str in vids_watched:\n",
    "            if vid_str in model.wv:\n",
    "                sum_vec += model.wv[vid_str]\n",
    "                count += 1\n",
    "        if count > 0:\n",
    "            user_emb[u_idx] = torch.tensor(sum_vec / count, dtype=torch.float32)\n",
    "    \n",
    "    return user_emb, video_emb\n",
    "\n",
    "# Step 2: Data Preparation\n",
    "with open(\"KuaiRec/adj_data.pkl\", \"rb\") as f:\n",
    "    adj_data = pickle.load(f)\n",
    "with open(\"KuaiRec/val_data.pkl\", \"rb\") as f:\n",
    "    val_data_dict = pickle.load(f)\n",
    "with open(\"KuaiRec/test_data.pkl\", \"rb\") as f:\n",
    "    test_data_dict = pickle.load(f)\n",
    "\n",
    "num_users = adj_data['num_nodes_dict']['user']\n",
    "num_items = adj_data['num_nodes_dict']['video']\n",
    "\n",
    "rowptr = adj_data['rowptr_dict'][('user', 'watches', 'video')]\n",
    "col = adj_data['col_dict'][('user', 'watches', 'video')]\n",
    "train_data = []\n",
    "for u in range(num_users):\n",
    "    start = rowptr[u]\n",
    "    end = rowptr[u + 1]\n",
    "    items = col[start:end].tolist()\n",
    "    train_data.extend([(u, i) for i in items])\n",
    "\n",
    "train_mat = sp.coo_matrix(\n",
    "    (np.ones(len(train_data)), ([x[0] for x in train_data], [x[1] for x in train_data])),\n",
    "    shape=(num_users, num_items)\n",
    ").tocsr()\n",
    "\n",
    "items_D = np.array(train_mat.sum(axis=0)).flatten()\n",
    "users_D = np.array(train_mat.sum(axis=1)).flatten()\n",
    "beta_uD = (np.sqrt(users_D + 1) / (users_D + 1e-8)).reshape(-1)\n",
    "beta_iD = (1 / np.sqrt(items_D + 1)).reshape(-1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "constraint_mat = {\n",
    "    \"beta_uD\": torch.from_numpy(beta_uD).float().to(device),\n",
    "    \"beta_iD\": torch.from_numpy(beta_iD).float().to(device)\n",
    "}\n",
    "\n",
    "def get_ii_constraint_mat(train_mat, num_neighbors, ii_diagonal_zero=False):\n",
    "    print('Computing item-item constraint matrix efficiently...')\n",
    "    A = train_mat.T.dot(train_mat).tocsr()\n",
    "    if ii_diagonal_zero:\n",
    "        A.setdiag(0)\n",
    "    A.eliminate_zeros()\n",
    "    n_items = A.shape[0]\n",
    "    items_D = np.array(A.sum(axis=0)).flatten()\n",
    "    users_D = np.array(A.sum(axis=1)).flatten()\n",
    "    beta_uD = (np.sqrt(users_D + 1) / (users_D + 1e-8)).reshape(-1, 1)\n",
    "    beta_iD = (1 / np.sqrt(items_D + 1)).reshape(1, -1)\n",
    "    all_ii_constraint_mat = beta_uD.dot(beta_iD)\n",
    "    res_mat = torch.zeros((n_items, num_neighbors), dtype=torch.long)\n",
    "    res_sim_mat = torch.zeros((n_items, num_neighbors), dtype=torch.float32)\n",
    "    batch_size = 10000\n",
    "    for start in range(0, n_items, batch_size):\n",
    "        end = min(start + batch_size, n_items)\n",
    "        batch_A = A[start:end].toarray()\n",
    "        batch_weighted = batch_A * all_ii_constraint_mat[start:end]\n",
    "        batch_tensor = torch.from_numpy(batch_weighted).float()\n",
    "        row_sims, row_idxs = torch.topk(batch_tensor, k=num_neighbors, dim=1)\n",
    "        res_mat[start:end] = row_idxs\n",
    "        res_sim_mat[start:end] = row_sims\n",
    "        print(f'Processed items {start} to {end-1}')\n",
    "    print('Item-item constraint matrix computed!')\n",
    "    return res_mat.to(device), res_sim_mat.to(device)\n",
    "\n",
    "ii_neighbor_num = 10\n",
    "ii_cons_mat_path = 'KuaiRec_ii_constraint_mat.pkl'\n",
    "ii_neigh_mat_path = 'KuaiRec_ii_neighbor_mat.pkl'\n",
    "if os.path.exists(ii_cons_mat_path) and os.path.exists(ii_neigh_mat_path):\n",
    "    with open(ii_cons_mat_path, 'rb') as f:\n",
    "        ii_constraint_mat = pickle.load(f).to(device)\n",
    "    with open(ii_neigh_mat_path, 'rb') as f:\n",
    "        ii_neighbor_mat = pickle.load(f).to(device)\n",
    "else:\n",
    "    ii_neighbor_mat, ii_constraint_mat = get_ii_constraint_mat(train_mat, ii_neighbor_num)\n",
    "    with open(ii_cons_mat_path, 'wb') as f:\n",
    "        pickle.dump(ii_constraint_mat, f)\n",
    "    with open(ii_neigh_mat_path, 'wb') as f:\n",
    "        pickle.dump(ii_neighbor_mat, f)\n",
    "\n",
    "interacted_items = [[] for _ in range(num_users)]\n",
    "for u, i in train_data:\n",
    "    interacted_items[u].append(i)\n",
    "\n",
    "mask = torch.zeros(num_users, num_items).to(device)\n",
    "for u, items in enumerate(interacted_items):\n",
    "    mask[u, items] = -np.inf\n",
    "\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.long)\n",
    "train_loader = DataLoader(\n",
    "    train_data_tensor, batch_size=8192, shuffle=True, num_workers=16, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    torch.tensor(list(val_data_dict.keys()), dtype=torch.long), batch_size=1024, shuffle=False, num_workers=5, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torch.tensor(list(test_data_dict.keys()), dtype=torch.long), batch_size=1024, shuffle=False, num_workers=5, pin_memory=True\n",
    ")\n",
    "\n",
    "# Step 2: Model Definition with W2V Initialization\n",
    "class UltraGCN(nn.Module):\n",
    "    def __init__(self, params, constraint_mat, ii_constraint_mat, ii_neighbor_mat, user_emb_init, item_emb_init):\n",
    "        super(UltraGCN, self).__init__()\n",
    "        self.user_num = params['user_num']\n",
    "        self.item_num = params['item_num']\n",
    "        self.embedding_dim = params['embedding_dim']\n",
    "        self.w1 = params['w1']\n",
    "        self.w2 = params['w2']\n",
    "        self.w3 = params['w3']\n",
    "        self.w4 = params['w4']\n",
    "        self.negative_weight = params['negative_weight']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_ = params['lambda']\n",
    "\n",
    "        # Initialize embeddings with W2V embeddings\n",
    "        self.user_embeds = nn.Embedding.from_pretrained(user_emb_init, freeze=False)\n",
    "        self.item_embeds = nn.Embedding.from_pretrained(item_emb_init, freeze=False)\n",
    "\n",
    "        self.constraint_mat = constraint_mat\n",
    "        self.ii_constraint_mat = ii_constraint_mat\n",
    "        self.ii_neighbor_mat = ii_neighbor_mat\n",
    "\n",
    "    def get_omegas(self, users, pos_items, neg_items):\n",
    "        device = self.get_device()\n",
    "        if self.w2 > 0:\n",
    "            pos_weight = torch.mul(self.constraint_mat['beta_uD'][users], self.constraint_mat['beta_iD'][pos_items])\n",
    "            pos_weight = self.w1 + self.w2 * pos_weight\n",
    "        else:\n",
    "            pos_weight = self.w1 * torch.ones(len(pos_items), device=device)\n",
    "        if self.w4 > 0:\n",
    "            neg_weight = torch.mul(\n",
    "                torch.repeat_interleave(self.constraint_mat['beta_uD'][users], neg_items.size(1)),\n",
    "                self.constraint_mat['beta_iD'][neg_items.flatten()]\n",
    "            )\n",
    "            neg_weight = self.w3 + self.w4 * neg_weight\n",
    "        else:\n",
    "            neg_weight = self.w3 * torch.ones(neg_items.size(0) * neg_items.size(1), device=device)\n",
    "        return torch.cat((pos_weight, neg_weight))\n",
    "\n",
    "    def cal_loss_L(self, users, pos_items, neg_items, omega_weight):\n",
    "        device = self.get_device()\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        pos_embeds = self.item_embeds(pos_items)\n",
    "        neg_embeds = self.item_embeds(neg_items)\n",
    "        pos_scores = (user_embeds * pos_embeds).sum(dim=-1)\n",
    "        user_embeds = user_embeds.unsqueeze(1)\n",
    "        neg_scores = (user_embeds * neg_embeds).sum(dim=-1)\n",
    "        neg_labels = torch.zeros(neg_scores.size(), device=device)\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(\n",
    "            neg_scores, neg_labels, weight=omega_weight[len(pos_scores):].view(neg_scores.size()), reduction='none'\n",
    "        ).mean(dim=-1)\n",
    "        pos_labels = torch.ones(pos_scores.size(), device=device)\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(\n",
    "            pos_scores, pos_labels, weight=omega_weight[:len(pos_scores)], reduction='none'\n",
    "        )\n",
    "        return (pos_loss + neg_loss * self.negative_weight).sum()\n",
    "\n",
    "    def cal_loss_I(self, users, pos_items):\n",
    "        device = self.get_device()\n",
    "        neighbor_embeds = self.item_embeds(self.ii_neighbor_mat[pos_items])\n",
    "        sim_scores = self.ii_constraint_mat[pos_items]\n",
    "        user_embeds = self.user_embeds(users).unsqueeze(1)\n",
    "        loss = -sim_scores * (user_embeds * neighbor_embeds).sum(dim=-1).sigmoid().log()\n",
    "        return loss.sum()\n",
    "\n",
    "    def norm_loss(self):\n",
    "        loss = 0.0\n",
    "        for parameter in self.parameters():\n",
    "            loss += torch.sum(parameter ** 2)\n",
    "        return loss / 2\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        omega_weight = self.get_omegas(users, pos_items, neg_items)\n",
    "        loss = self.cal_loss_L(users, pos_items, neg_items, omega_weight)\n",
    "        loss += self.gamma * self.norm_loss()\n",
    "        loss += self.lambda_ * self.cal_loss_I(users, pos_items)\n",
    "        return loss\n",
    "\n",
    "    def test_forward(self, users):\n",
    "        items = torch.arange(self.item_num).to(users.device)\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        item_embeds = self.item_embeds(items)\n",
    "        return user_embeds.mm(item_embeds.t())\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.user_embeds.weight.device\n",
    "\n",
    "# Step 3: Training Loop\n",
    "params = {\n",
    "    'user_num': num_users,\n",
    "    'item_num': num_items,\n",
    "    'embedding_dim': 50,\n",
    "    'w1': 1.0,\n",
    "    'w2': 1.0,\n",
    "    'w3': 1.0,\n",
    "    'w4': 1.0,\n",
    "    'negative_weight': 1.0,\n",
    "    'gamma': 0.001,\n",
    "    'lambda': 0.0001,\n",
    "    'initial_weight': 0.1,\n",
    "    'lr': 0.005,\n",
    "    'batch_size': 8192,\n",
    "    'max_epoch': 100,\n",
    "    'early_stop_epoch': 10,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'topk': [1,2,3,5,10,20,30,40,50,60,70,80,90,100],\n",
    "    'negative_num': 1,\n",
    "    'sampling_sift_pos': True,\n",
    "    'is_validation': True\n",
    "}\n",
    "\n",
    "def Sampling(users, pos_items, item_num, neg_ratio, interacted_items, sampling_sift_pos):\n",
    "    neg_candidates = np.arange(item_num)\n",
    "    if sampling_sift_pos:\n",
    "        neg_items = []\n",
    "        for u in users:\n",
    "            probs = np.ones(item_num)\n",
    "            probs[interacted_items[u]] = 0\n",
    "            probs /= np.sum(probs)\n",
    "            u_neg_items = np.random.choice(neg_candidates, size=neg_ratio, p=probs, replace=True)\n",
    "            neg_items.append(u_neg_items)\n",
    "        neg_items = np.array(neg_items)\n",
    "    else:\n",
    "        neg_items = np.random.choice(neg_candidates, (len(users), neg_ratio), replace=True)\n",
    "    return users, pos_items, torch.from_numpy(neg_items).long().to(users.device)\n",
    "\n",
    "def evaluate(model, loader, ground_truth_dict, mask, other_dict, top_k, is_validation=True):\n",
    "    total_recall = {k: 0.0 for k in top_k}\n",
    "    total_ndcg = {k: 0.0 for k in top_k}\n",
    "    recommended_items = {k: set() for k in top_k}\n",
    "    total_items = model.item_num\n",
    "    user_count = 0\n",
    "\n",
    "    idcg_cache = {}\n",
    "    def get_idcg(r_size, k):\n",
    "        cut = min(k, r_size)\n",
    "        if (cut, k) not in idcg_cache:\n",
    "            val = 0.0\n",
    "            for i in range(cut):\n",
    "                val += 1.0 / np.log2(i + 2)\n",
    "            idcg_cache[(cut, k)] = val\n",
    "        return idcg_cache[(cut, k)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_users in loader:\n",
    "            batch_users = batch_users.to(model.get_device())\n",
    "            rating = model.test_forward(batch_users)\n",
    "            rating += mask[batch_users]\n",
    "            rating = rating.cpu().numpy()\n",
    "\n",
    "            for i, u in enumerate(batch_users.cpu().numpy()):\n",
    "                relevant_set = set(ground_truth_dict.get(u, []))\n",
    "                if not relevant_set:\n",
    "                    continue\n",
    "                other_set = set(other_dict.get(u, []))\n",
    "                scores = rating[i]\n",
    "                exclude_items = set(interacted_items[u]) | other_set\n",
    "                valid_items = [vid for vid in range(total_items) if vid not in exclude_items]\n",
    "                scores_filtered = scores[valid_items]\n",
    "                ranked_items = np.argsort(-scores_filtered)[:max(top_k)]\n",
    "                ranked_items = [valid_items[vid] for vid in ranked_items]\n",
    "\n",
    "                hits_positions = [pos for pos, vid in enumerate(ranked_items) if vid in relevant_set]\n",
    "\n",
    "                for k in top_k:\n",
    "                    hits_count = sum(1 for pos in hits_positions if pos < k)\n",
    "                    recall_k = hits_count / float(len(relevant_set))\n",
    "                    total_recall[k] += recall_k\n",
    "\n",
    "                    dcg_val = sum(1.0 / np.log2(pos + 2) for pos in hits_positions if pos < k)\n",
    "                    idcg_val = get_idcg(len(relevant_set), k)\n",
    "                    ndcg_k = (dcg_val / idcg_val) if idcg_val > 0 else 0.0\n",
    "                    total_ndcg[k] += ndcg_k\n",
    "\n",
    "                    recommended_items[k].update(ranked_items[:k])\n",
    "                user_count += 1\n",
    "\n",
    "    if user_count == 0:\n",
    "        return {f'Recall@{k}': 0.0 for k in top_k} | {f'NDCG@{k}': 0.0 for k in top_k} | {f'Coverage@{k}': 0.0 for k in top_k}\n",
    "\n",
    "    avg_recall = {f'Recall@{k}': total_recall[k] / user_count for k in top_k}\n",
    "    avg_ndcg = {f'NDCG@{k}': total_ndcg[k] / user_count for k in top_k}\n",
    "    coverage = {f'Coverage@{k}': len(recommended_items[k]) / total_items for k in top_k}\n",
    "    return avg_recall | avg_ndcg | coverage\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, test_loader, mask, val_data_dict, test_data_dict, interacted_items, params):\n",
    "    device = params['device']\n",
    "    best_recall, best_epoch = 0, 0\n",
    "    early_stop_count = 0\n",
    "    patience = 3\n",
    "\n",
    "    for epoch in range(params['max_epoch']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            users, pos_items = batch[:, 0], batch[:, 1]\n",
    "            users, pos_items, neg_items = Sampling(\n",
    "                users, pos_items, params['item_num'], params['negative_num'], interacted_items, params['sampling_sift_pos']\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(users, pos_items, neg_items)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}')\n",
    "\n",
    "        test_metrics = evaluate(model, val_loader, val_data_dict, mask, test_data_dict if params['is_validation'] else val_data_dict, params['topk'], is_validation=False)\n",
    "        recall_at_k = test_metrics[f'Recall@{max(params[\"topk\"])}']\n",
    "        print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "        if recall_at_k > best_recall:\n",
    "            best_recall, best_epoch = recall_at_k, epoch\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                print(f'Early stop at epoch {epoch+1}, Best Recall@{max(params[\"topk\"])}: {best_recall:.4f} at epoch {best_epoch+1}')\n",
    "                break\n",
    "\n",
    "    #test_metrics = evaluate(model, test_loader, test_data_dict, mask, val_data_dict if params['is_validation'] else test_data_dict, params['topk'], is_validation=False)\n",
    "    #print(f\"Final Test Metrics: {test_metrics}\")\n",
    "    #return test_metrics\n",
    "\n",
    "# Step 5: Run Training with W2V Initialization\n",
    "user_emb_init, item_emb_init = w2v_emb(embedding_dim=params['embedding_dim'])\n",
    "model = UltraGCN(params, constraint_mat, ii_constraint_mat, ii_neighbor_mat, user_emb_init, item_emb_init)\n",
    "model = model.to(params['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "train(model, optimizer, train_loader, val_loader, test_loader, mask, val_data_dict, test_data_dict, interacted_items, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "827a263c-8aaf-4455-9919-d6153ede8cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11151.8186, Validation Metrics: {'Recall@1': 0.011458258828346259, 'Recall@2': 0.020400784553366993, 'Recall@3': 0.02919536287847556, 'Recall@5': 0.042889765728618, 'Recall@10': 0.06455385814906589, 'Recall@20': 0.09404348931488998, 'Recall@30': 0.1169038920067873, 'Recall@40': 0.13386393963555884, 'Recall@50': 0.1466741898961528, 'Recall@60': 0.15957888056991373, 'Recall@70': 0.16950628637928114, 'Recall@80': 0.18006949710091477, 'Recall@90': 0.1883240968726763, 'Recall@100': 0.19523850775447504, 'NDCG@1': 0.1261516654854713, 'NDCG@2': 0.11672872338279806, 'NDCG@3': 0.11267452227568862, 'NDCG@5': 0.10771109849455136, 'NDCG@10': 0.10192417971625058, 'NDCG@20': 0.10439573538819427, 'NDCG@30': 0.10943588573133388, 'NDCG@40': 0.1139148226204892, 'NDCG@50': 0.11757951591212351, 'NDCG@60': 0.12172282364423176, 'NDCG@70': 0.1252918764909931, 'NDCG@80': 0.12896830013756416, 'NDCG@90': 0.13215450901708126, 'NDCG@100': 0.1348304212463064, 'Coverage@1': 0.029921700223713647, 'Coverage@2': 0.05480984340044743, 'Coverage@3': 0.07876584638329605, 'Coverage@5': 0.12136465324384788, 'Coverage@10': 0.21140939597315436, 'Coverage@20': 0.35272184936614465, 'Coverage@30': 0.4565622669649515, 'Coverage@40': 0.540268456375839, 'Coverage@50': 0.6087807606263982, 'Coverage@60': 0.6621923937360179, 'Coverage@70': 0.7115958240119314, 'Coverage@80': 0.7478560775540641, 'Coverage@90': 0.7811334824757643, 'Coverage@100': 0.8090044742729307}\n",
      "Epoch 2, Loss: 7352.3536, Validation Metrics: {'Recall@1': 0.02005460183880451, 'Recall@2': 0.03592174557311276, 'Recall@3': 0.04616237477553958, 'Recall@5': 0.06146424723852795, 'Recall@10': 0.08371859625654751, 'Recall@20': 0.12273670585885475, 'Recall@30': 0.1542731458426025, 'Recall@40': 0.1755284674262601, 'Recall@50': 0.19592054881885204, 'Recall@60': 0.21219716560669227, 'Recall@70': 0.22769812820420393, 'Recall@80': 0.24225927203264896, 'Recall@90': 0.2557639022372308, 'Recall@100': 0.26731737793139587, 'NDCG@1': 0.2069454287739192, 'NDCG@2': 0.19652699739552615, 'NDCG@3': 0.17943761470715774, 'NDCG@5': 0.16097270795807392, 'NDCG@10': 0.14173767843350488, 'NDCG@20': 0.14119209609484723, 'NDCG@30': 0.14755945593281733, 'NDCG@40': 0.15261601612128237, 'NDCG@50': 0.15880867690135625, 'NDCG@60': 0.1641732034930173, 'NDCG@70': 0.16956748572605057, 'NDCG@80': 0.17486813458456182, 'NDCG@90': 0.1798150527798118, 'NDCG@100': 0.18428634752100087, 'Coverage@1': 0.0041946308724832215, 'Coverage@2': 0.00633855331841909, 'Coverage@3': 0.008296047725577927, 'Coverage@5': 0.011558538404175988, 'Coverage@10': 0.01948173005219985, 'Coverage@20': 0.034395973154362415, 'Coverage@30': 0.047818791946308725, 'Coverage@40': 0.05965697240865026, 'Coverage@50': 0.07252050708426547, 'Coverage@60': 0.0837994034302759, 'Coverage@70': 0.09619686800894854, 'Coverage@80': 0.10868754660700969, 'Coverage@90': 0.12052572706935123, 'Coverage@100': 0.13143176733780762}\n",
      "Epoch 3, Loss: 6391.8953, Validation Metrics: {'Recall@1': 0.006893281164285506, 'Recall@2': 0.014978924735539908, 'Recall@3': 0.020648706616977524, 'Recall@5': 0.0323168356395498, 'Recall@10': 0.05160147780240118, 'Recall@20': 0.08134517945556727, 'Recall@30': 0.10455959510298975, 'Recall@40': 0.1287408101113398, 'Recall@50': 0.15289834748706288, 'Recall@60': 0.17237686181534764, 'Recall@70': 0.18779368918264117, 'Recall@80': 0.20038950321671942, 'Recall@90': 0.20926866938689473, 'Recall@100': 0.21829732073508484, 'NDCG@1': 0.07654145995747696, 'NDCG@2': 0.07544478297027772, 'NDCG@3': 0.07314338130028153, 'NDCG@5': 0.07271810616653243, 'NDCG@10': 0.07115324541686376, 'NDCG@20': 0.0771850690298507, 'NDCG@30': 0.08409244081890693, 'NDCG@40': 0.092038471873939, 'NDCG@50': 0.10030890093755428, 'NDCG@60': 0.10687017100564755, 'NDCG@70': 0.11222135051878206, 'NDCG@80': 0.11659819203518401, 'NDCG@90': 0.11979404430742938, 'NDCG@100': 0.12304362094723686, 'Coverage@1': 0.0038217747949291572, 'Coverage@2': 0.006058911260253542, 'Coverage@3': 0.007829977628635347, 'Coverage@5': 0.011651752423564504, 'Coverage@10': 0.018642803877703208, 'Coverage@20': 0.029455630126771066, 'Coverage@30': 0.039802386278896346, 'Coverage@40': 0.0494034302759135, 'Coverage@50': 0.058165548098434, 'Coverage@60': 0.06590231170768084, 'Coverage@70': 0.0738255033557047, 'Coverage@80': 0.08156226696495153, 'Coverage@90': 0.09051081282624907, 'Coverage@100': 0.09899328859060402}\n",
      "Epoch 4, Loss: 6006.6435, Validation Metrics: {'Recall@1': 0.009356614458711039, 'Recall@2': 0.01901842820300931, 'Recall@3': 0.025663786922636173, 'Recall@5': 0.032189983839404836, 'Recall@10': 0.05076942458036588, 'Recall@20': 0.08072341481314559, 'Recall@30': 0.10389795159819723, 'Recall@40': 0.12401043433000916, 'Recall@50': 0.1439903352397313, 'Recall@60': 0.1592713037282636, 'Recall@70': 0.17320101525558845, 'Recall@80': 0.18675951686968423, 'Recall@90': 0.19788974888699742, 'Recall@100': 0.20615602315265424, 'NDCG@1': 0.09780297661233169, 'NDCG@2': 0.09286793016993494, 'NDCG@3': 0.08824326397426661, 'NDCG@5': 0.0772998501217236, 'NDCG@10': 0.07260959612185296, 'NDCG@20': 0.07887142689105639, 'NDCG@30': 0.08612111428657285, 'NDCG@40': 0.09268659672391835, 'NDCG@50': 0.09939228965214517, 'NDCG@60': 0.10463185946327673, 'NDCG@70': 0.10947414217417532, 'NDCG@80': 0.11410009553616329, 'NDCG@90': 0.11764255685929595, 'NDCG@100': 0.1206421186506488, 'Coverage@1': 0.005219985085756898, 'Coverage@2': 0.0068046234153616705, 'Coverage@3': 0.008855331841909023, 'Coverage@5': 0.012863534675615212, 'Coverage@10': 0.02115958240119314, 'Coverage@20': 0.032997762863534674, 'Coverage@30': 0.0441834451901566, 'Coverage@40': 0.05453020134228188, 'Coverage@50': 0.06562266964951528, 'Coverage@60': 0.07475764354958986, 'Coverage@70': 0.08398583146905295, 'Coverage@80': 0.09265473527218494, 'Coverage@90': 0.1017897091722595, 'Coverage@100': 0.11167039522744221}\n",
      "Epoch 5, Loss: 5717.2958, Validation Metrics: {'Recall@1': 0.006921658737904913, 'Recall@2': 0.013121654130481369, 'Recall@3': 0.020123637272052963, 'Recall@5': 0.02851517589123087, 'Recall@10': 0.0433152675051036, 'Recall@20': 0.0710613518475332, 'Recall@30': 0.09757230179738012, 'Recall@40': 0.12221609944654513, 'Recall@50': 0.14036004355444584, 'Recall@60': 0.15746580766956197, 'Recall@70': 0.1720523598534495, 'Recall@80': 0.1830075173110324, 'Recall@90': 0.1923783089186668, 'Recall@100': 0.20188124920003606, 'NDCG@1': 0.07087172218284904, 'NDCG@2': 0.0620983062852549, 'NDCG@3': 0.061560538580566204, 'NDCG@5': 0.058201709249076194, 'NDCG@10': 0.05649306831229395, 'NDCG@20': 0.06393201785875177, 'NDCG@30': 0.07365421713608146, 'NDCG@40': 0.08220958851850091, 'NDCG@50': 0.08860709785949407, 'NDCG@60': 0.09442623554076982, 'NDCG@70': 0.0995438756927077, 'NDCG@80': 0.10349223657952483, 'NDCG@90': 0.10701372127827734, 'NDCG@100': 0.11031469966770052, 'Coverage@1': 0.00540641312453393, 'Coverage@2': 0.00727069351230425, 'Coverage@3': 0.009601043997017151, 'Coverage@5': 0.013516032811334825, 'Coverage@10': 0.023583146905294557, 'Coverage@20': 0.040548098434004476, 'Coverage@30': 0.053970917225950786, 'Coverage@40': 0.06674123788217748, 'Coverage@50': 0.07829977628635347, 'Coverage@60': 0.09032438478747204, 'Coverage@70': 0.10234899328859061, 'Coverage@80': 0.11344146159582401, 'Coverage@90': 0.12313571961222967, 'Coverage@100': 0.13217747949291572}\n",
      "Finished training, Best Recall@100: 0.2673 at epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.26731737793139587, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without W2V init\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "with open(\"KuaiRec/adj_data.pkl\", \"rb\") as f:\n",
    "    adj_data = pickle.load(f)\n",
    "with open(\"KuaiRec/val_data.pkl\", \"rb\") as f:\n",
    "    val_data_dict = pickle.load(f)\n",
    "with open(\"KuaiRec/test_data.pkl\", \"rb\") as f:\n",
    "    test_data_dict = pickle.load(f)\n",
    "\n",
    "num_users = adj_data['num_nodes_dict']['user']\n",
    "num_items = adj_data['num_nodes_dict']['video']\n",
    "\n",
    "rowptr = adj_data['rowptr_dict'][('user', 'watches', 'video')]\n",
    "col = adj_data['col_dict'][('user', 'watches', 'video')]\n",
    "train_data = []\n",
    "for u in range(num_users):\n",
    "    start = rowptr[u]\n",
    "    end = rowptr[u + 1]\n",
    "    items = col[start:end].tolist()\n",
    "    train_data.extend([(u, i) for i in items])\n",
    "\n",
    "train_mat = sp.coo_matrix(\n",
    "    (np.ones(len(train_data)), ([x[0] for x in train_data], [x[1] for x in train_data])),\n",
    "    shape=(num_users, num_items)\n",
    ").tocsr()\n",
    "\n",
    "items_D = np.array(train_mat.sum(axis=0)).flatten()\n",
    "users_D = np.array(train_mat.sum(axis=1)).flatten()\n",
    "beta_uD = (np.sqrt(users_D + 1) / (users_D + 1e-8)).reshape(-1)\n",
    "beta_iD = (1 / np.sqrt(items_D + 1)).reshape(-1)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "constraint_mat = {\n",
    "    \"beta_uD\": torch.from_numpy(beta_uD).float().to(device),\n",
    "    \"beta_iD\": torch.from_numpy(beta_iD).float().to(device)\n",
    "}\n",
    "\n",
    "def get_ii_constraint_mat(train_mat, num_neighbors, ii_diagonal_zero=False):\n",
    "    print('Computing item-item constraint matrix efficiently...')\n",
    "    A = train_mat.T.dot(train_mat).tocsr()\n",
    "    if ii_diagonal_zero:\n",
    "        A.setdiag(0)\n",
    "    A.eliminate_zeros()\n",
    "    n_items = A.shape[0]\n",
    "    items_D = np.array(A.sum(axis=0)).flatten()\n",
    "    users_D = np.array(A.sum(axis=1)).flatten()\n",
    "    beta_uD = (np.sqrt(users_D + 1) / (users_D + 1e-8)).reshape(-1, 1)\n",
    "    beta_iD = (1 / np.sqrt(items_D + 1)).reshape(1, -1)\n",
    "    all_ii_constraint_mat = beta_uD.dot(beta_iD)\n",
    "    res_mat = torch.zeros((n_items, num_neighbors), dtype=torch.long)\n",
    "    res_sim_mat = torch.zeros((n_items, num_neighbors), dtype=torch.float32)\n",
    "    batch_size = 10000\n",
    "    for start in range(0, n_items, batch_size):\n",
    "        end = min(start + batch_size, n_items)\n",
    "        batch_A = A[start:end].toarray()\n",
    "        batch_weighted = batch_A * all_ii_constraint_mat[start:end]\n",
    "        batch_tensor = torch.from_numpy(batch_weighted).float()\n",
    "        row_sims, row_idxs = torch.topk(batch_tensor, k=num_neighbors, dim=1)\n",
    "        res_mat[start:end] = row_idxs\n",
    "        res_sim_mat[start:end] = row_sims\n",
    "        print(f'Processed items {start} to {end-1}')\n",
    "    print('Item-item constraint matrix computed!')\n",
    "    return res_mat.to(device), res_sim_mat.to(device)\n",
    "\n",
    "ii_neighbor_num = 10\n",
    "ii_cons_mat_path = 'KuaiRec_ii_constraint_mat.pkl'\n",
    "ii_neigh_mat_path = 'KuaiRec_ii_neighbor_mat.pkl'\n",
    "if os.path.exists(ii_cons_mat_path) and os.path.exists(ii_neigh_mat_path):\n",
    "    with open(ii_cons_mat_path, 'rb') as f:\n",
    "        ii_constraint_mat = pickle.load(f).to(device)\n",
    "    with open(ii_neigh_mat_path, 'rb') as f:\n",
    "        ii_neighbor_mat = pickle.load(f).to(device)\n",
    "else:\n",
    "    ii_neighbor_mat, ii_constraint_mat = get_ii_constraint_mat(train_mat, ii_neighbor_num)\n",
    "    with open(ii_cons_mat_path, 'wb') as f:\n",
    "        pickle.dump(ii_constraint_mat, f)\n",
    "    with open(ii_neigh_mat_path, 'wb') as f:\n",
    "        pickle.dump(ii_neighbor_mat, f)\n",
    "\n",
    "interacted_items = [[] for _ in range(num_users)]\n",
    "for u, i in train_data:\n",
    "    interacted_items[u].append(i)\n",
    "\n",
    "mask = torch.zeros(num_users, num_items).to(device)\n",
    "for u, items in enumerate(interacted_items):\n",
    "    mask[u, items] = -np.inf\n",
    "\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.long)\n",
    "train_loader = DataLoader(\n",
    "    train_data_tensor, batch_size=8192, shuffle=True, num_workers=16, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    torch.tensor(list(val_data_dict.keys()), dtype=torch.long), batch_size=1024, shuffle=False, num_workers=5, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    torch.tensor(list(test_data_dict.keys()), dtype=torch.long), batch_size=1024, shuffle=False, num_workers=5, pin_memory=True\n",
    ")\n",
    "\n",
    "# Step 2: Model Definition\n",
    "class UltraGCN(nn.Module):\n",
    "    def __init__(self, params, constraint_mat, ii_constraint_mat, ii_neighbor_mat):\n",
    "        super(UltraGCN, self).__init__()\n",
    "        self.user_num = params['user_num']\n",
    "        self.item_num = params['item_num']\n",
    "        self.embedding_dim = params['embedding_dim']\n",
    "        self.w1 = params['w1']\n",
    "        self.w2 = params['w2']\n",
    "        self.w3 = params['w3']\n",
    "        self.w4 = params['w4']\n",
    "        self.negative_weight = params['negative_weight']\n",
    "        self.gamma = params['gamma']\n",
    "        self.lambda_ = params['lambda']\n",
    "\n",
    "        self.user_embeds = nn.Embedding(self.user_num, self.embedding_dim)\n",
    "        self.item_embeds = nn.Embedding(self.item_num, self.embedding_dim)\n",
    "\n",
    "        self.constraint_mat = constraint_mat\n",
    "        self.ii_constraint_mat = ii_constraint_mat\n",
    "        self.ii_neighbor_mat = ii_neighbor_mat\n",
    "\n",
    "        self.initial_weight = params['initial_weight']\n",
    "        self.initial_weights()\n",
    "\n",
    "    def initial_weights(self):\n",
    "        nn.init.normal_(self.user_embeds.weight, std=self.initial_weight)\n",
    "        nn.init.normal_(self.item_embeds.weight, std=self.initial_weight)\n",
    "\n",
    "    def get_omegas(self, users, pos_items, neg_items):\n",
    "        device = self.get_device()\n",
    "        if self.w2 > 0:\n",
    "            pos_weight = torch.mul(self.constraint_mat['beta_uD'][users], self.constraint_mat['beta_iD'][pos_items])\n",
    "            pos_weight = self.w1 + self.w2 * pos_weight\n",
    "        else:\n",
    "            pos_weight = self.w1 * torch.ones(len(pos_items), device=device)\n",
    "        if self.w4 > 0:\n",
    "            neg_weight = torch.mul(\n",
    "                torch.repeat_interleave(self.constraint_mat['beta_uD'][users], neg_items.size(1)),\n",
    "                self.constraint_mat['beta_iD'][neg_items.flatten()]\n",
    "            )\n",
    "            neg_weight = self.w3 + self.w4 * neg_weight\n",
    "        else:\n",
    "            neg_weight = self.w3 * torch.ones(neg_items.size(0) * neg_items.size(1), device=device)\n",
    "        return torch.cat((pos_weight, neg_weight))\n",
    "\n",
    "    def cal_loss_L(self, users, pos_items, neg_items, omega_weight):\n",
    "        device = self.get_device()\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        pos_embeds = self.item_embeds(pos_items)\n",
    "        neg_embeds = self.item_embeds(neg_items)\n",
    "        pos_scores = (user_embeds * pos_embeds).sum(dim=-1)\n",
    "        user_embeds = user_embeds.unsqueeze(1)\n",
    "        neg_scores = (user_embeds * neg_embeds).sum(dim=-1)\n",
    "        neg_labels = torch.zeros(neg_scores.size(), device=device)\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(\n",
    "            neg_scores, neg_labels, weight=omega_weight[len(pos_scores):].view(neg_scores.size()), reduction='none'\n",
    "        ).mean(dim=-1)\n",
    "        pos_labels = torch.ones(pos_scores.size(), device=device)\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(\n",
    "            pos_scores, pos_labels, weight=omega_weight[:len(pos_scores)], reduction='none'\n",
    "        )\n",
    "        return (pos_loss + neg_loss * self.negative_weight).sum()\n",
    "\n",
    "    def cal_loss_I(self, users, pos_items):\n",
    "        device = self.get_device()\n",
    "        neighbor_embeds = self.item_embeds(self.ii_neighbor_mat[pos_items])\n",
    "        sim_scores = self.ii_constraint_mat[pos_items]\n",
    "        user_embeds = self.user_embeds(users).unsqueeze(1)\n",
    "        loss = -sim_scores * (user_embeds * neighbor_embeds).sum(dim=-1).sigmoid().log()\n",
    "        return loss.sum()\n",
    "\n",
    "    def norm_loss(self):\n",
    "        loss = 0.0\n",
    "        for parameter in self.parameters():\n",
    "            loss += torch.sum(parameter ** 2)\n",
    "        return loss / 2\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        omega_weight = self.get_omegas(users, pos_items, neg_items)\n",
    "        loss = self.cal_loss_L(users, pos_items, neg_items, omega_weight)\n",
    "        loss += self.gamma * self.norm_loss()\n",
    "        loss += self.lambda_ * self.cal_loss_I(users, pos_items)\n",
    "        return loss\n",
    "\n",
    "    def test_forward(self, users):\n",
    "        items = torch.arange(self.item_num).to(users.device)\n",
    "        user_embeds = self.user_embeds(users)\n",
    "        item_embeds = self.item_embeds(items)\n",
    "        return user_embeds.mm(item_embeds.t())\n",
    "\n",
    "    def get_device(self):\n",
    "        return self.user_embeds.weight.device\n",
    "\n",
    "# Step 3: Training Loop\n",
    "params = {\n",
    "    'user_num': num_users,\n",
    "    'item_num': num_items,\n",
    "    'embedding_dim': 50,\n",
    "    'w1': 1.0,\n",
    "    'w2': 1.0,\n",
    "    'w3': 1.0,\n",
    "    'w4': 1.0,\n",
    "    'negative_weight': 1.0,\n",
    "    'gamma': 0.001,\n",
    "    'lambda': 0.0001,\n",
    "    'initial_weight': 0.1,\n",
    "    'lr': 0.005,\n",
    "    'batch_size': 8192,\n",
    "    'max_epoch': 100,\n",
    "    'early_stop_epoch': 10,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'topk': [1,2,3,5,10,20,30,40,50,60,70,80,90,100],\n",
    "    'negative_num': 1,\n",
    "    'sampling_sift_pos': True,\n",
    "    'is_validation': True\n",
    "}\n",
    "\n",
    "def Sampling(users, pos_items, item_num, neg_ratio, interacted_items, sampling_sift_pos):\n",
    "    neg_candidates = np.arange(item_num)\n",
    "    if sampling_sift_pos:\n",
    "        neg_items = []\n",
    "        for u in users:\n",
    "            probs = np.ones(item_num)\n",
    "            probs[interacted_items[u]] = 0\n",
    "            probs /= np.sum(probs)\n",
    "            u_neg_items = np.random.choice(neg_candidates, size=neg_ratio, p=probs, replace=True)\n",
    "            neg_items.append(u_neg_items)\n",
    "        neg_items = np.array(neg_items)\n",
    "    else:\n",
    "        neg_items = np.random.choice(neg_candidates, (len(users), neg_ratio), replace=True)\n",
    "    return users, pos_items, torch.from_numpy(neg_items).long().to(users.device)\n",
    "\n",
    "def evaluate(model, loader, ground_truth_dict, mask, other_dict, top_k, is_validation=True):\n",
    "    total_recall = {k: 0.0 for k in top_k}\n",
    "    total_ndcg = {k: 0.0 for k in top_k}\n",
    "    recommended_items = {k: set() for k in top_k}\n",
    "    total_items = model.item_num\n",
    "    user_count = 0\n",
    "\n",
    "    idcg_cache = {}\n",
    "    def get_idcg(r_size, k):\n",
    "        cut = min(k, r_size)\n",
    "        if (cut, k) not in idcg_cache:\n",
    "            val = 0.0\n",
    "            for i in range(cut):\n",
    "                val += 1.0 / np.log2(i + 2)\n",
    "            idcg_cache[(cut, k)] = val\n",
    "        return idcg_cache[(cut, k)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for batch_users in loader:\n",
    "            batch_users = batch_users.to(model.get_device())\n",
    "            rating = model.test_forward(batch_users)  # Shape: [batch_size, item_num]\n",
    "            rating += mask[batch_users]  # Mask training items\n",
    "            rating = rating.cpu().numpy()\n",
    "\n",
    "            for i, u in enumerate(batch_users.cpu().numpy()):\n",
    "                relevant_set = set(ground_truth_dict.get(u, []))\n",
    "                if not relevant_set:\n",
    "                    continue\n",
    "                other_set = set(other_dict.get(u, []))\n",
    "                scores = rating[i]\n",
    "                # Exclude training and other_set items\n",
    "                exclude_items = set(interacted_items[u]) | other_set\n",
    "                valid_items = [vid for vid in range(total_items) if vid not in exclude_items]\n",
    "                scores_filtered = scores[valid_items]\n",
    "                ranked_items = np.argsort(-scores_filtered)[:max(top_k)]  # Top-k after exclusion\n",
    "                ranked_items = [valid_items[vid] for vid in ranked_items]\n",
    "\n",
    "                hits_positions = [pos for pos, vid in enumerate(ranked_items) if vid in relevant_set]\n",
    "\n",
    "                for k in top_k:\n",
    "                    hits_count = sum(1 for pos in hits_positions if pos < k)\n",
    "                    recall_k = hits_count / float(len(relevant_set))\n",
    "                    total_recall[k] += recall_k\n",
    "\n",
    "                    dcg_val = sum(1.0 / np.log2(pos + 2) for pos in hits_positions if pos < k)\n",
    "                    idcg_val = get_idcg(len(relevant_set), k)\n",
    "                    ndcg_k = (dcg_val / idcg_val) if idcg_val > 0 else 0.0\n",
    "                    total_ndcg[k] += ndcg_k\n",
    "\n",
    "                    recommended_items[k].update(ranked_items[:k])\n",
    "                user_count += 1\n",
    "\n",
    "    if user_count == 0:\n",
    "        return {f'Recall@{k}': 0.0 for k in top_k} | {f'NDCG@{k}': 0.0 for k in top_k} | {f'Coverage@{k}': 0.0 for k in top_k}\n",
    "\n",
    "    avg_recall = {f'Recall@{k}': total_recall[k] / user_count for k in top_k}\n",
    "    avg_ndcg = {f'NDCG@{k}': total_ndcg[k] / user_count for k in top_k}\n",
    "    coverage = {f'Coverage@{k}': len(recommended_items[k]) / total_items for k in top_k}\n",
    "    return avg_recall | avg_ndcg | coverage\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, test_loader, mask, val_data_dict, test_data_dict, interacted_items, params):\n",
    "    device = params['device']\n",
    "    best_recall, best_epoch = 0, 0\n",
    "    early_stop_count = 0\n",
    "    patience = 3  # Stop if no improvement for 3 epochs\n",
    "\n",
    "    for epoch in range(params['max_epoch']):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            users, pos_items = batch[:, 0], batch[:, 1]\n",
    "            users, pos_items, neg_items = Sampling(\n",
    "                users, pos_items, params['item_num'], params['negative_num'], interacted_items, params['sampling_sift_pos']\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(users, pos_items, neg_items)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_metrics = evaluate(model, val_loader, val_data_dict, mask, test_data_dict if params['is_validation'] else val_data_dict, params['topk'], is_validation=False)\n",
    "        recall_at_k = val_metrics[f'Recall@{max(params[\"topk\"])}']\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Validation Metrics: {val_metrics}\")\n",
    "\n",
    "        if recall_at_k > best_recall:\n",
    "            best_recall, best_epoch = recall_at_k, epoch\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "            if early_stop_count >= patience:\n",
    "                #print(f'Early stop at epoch {epoch+1}, Best Recall@{params[\"topk\"]}: {best_recall:.4f} at epoch {best_epoch+1}')\n",
    "                break\n",
    "    print(f'Finished training, Best Recall@{max(params[\"topk\"])}: {best_recall:.4f} at epoch {best_epoch+1}')\n",
    "    return best_recall, best_epoch\n",
    "\n",
    "\n",
    "# Step 5: Run Training\n",
    "model = UltraGCN(params, constraint_mat, ii_constraint_mat, ii_neighbor_mat)\n",
    "model = model.to(params['device'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "train(model, optimizer, train_loader, val_loader, test_loader, mask, val_data_dict, test_data_dict, interacted_items, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b71b330-5042-49a6-a216-d66d1fcf3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ultragcn_metrics = {\n",
    "    'Recall@1': 0.02005460183880451,\n",
    "    'Recall@2': 0.03592174557311276,\n",
    "    'Recall@3': 0.04616237477553958,\n",
    "    'Recall@5': 0.06146424723852795,\n",
    "    'Recall@10': 0.08371859625654751,\n",
    "    'Recall@20': 0.12273670585885475,\n",
    "    'Recall@30': 0.1542731458426025,\n",
    "    'Recall@40': 0.1755284674262601,\n",
    "    'Recall@50': 0.19592054881885204,\n",
    "    'Recall@60': 0.21219716560669227,\n",
    "    'Recall@70': 0.22769812820420393,\n",
    "    'Recall@80': 0.24225927203264896,\n",
    "    'Recall@90': 0.2557639022372308,\n",
    "    'Recall@100': 0.26731737793139587,\n",
    "    'NDCG@1': 0.2069454287739192,\n",
    "    'NDCG@2': 0.19652699739552615,\n",
    "    'NDCG@3': 0.17943761470715774,\n",
    "    'NDCG@5': 0.16097270795807392,\n",
    "    'NDCG@10': 0.14173767843350488,\n",
    "    'NDCG@20': 0.14119209609484723,\n",
    "    'NDCG@30': 0.14755945593281733,\n",
    "    'NDCG@40': 0.15261601612128237,\n",
    "    'NDCG@50': 0.15880867690135625,\n",
    "    'NDCG@60': 0.1641732034930173,\n",
    "    'NDCG@70': 0.16956748572605057,\n",
    "    'NDCG@80': 0.17486813458456182,\n",
    "    'NDCG@90': 0.1798150527798118,\n",
    "    'NDCG@100': 0.18428634752100087,\n",
    "    'Coverage@1': 0.0041946308724832215,\n",
    "    'Coverage@2': 0.00633855331841909,\n",
    "    'Coverage@3': 0.008296047725577927,\n",
    "    'Coverage@5': 0.011558538404175988,\n",
    "    'Coverage@10': 0.01948173005219985,\n",
    "    'Coverage@20': 0.034395973154362415,\n",
    "    'Coverage@30': 0.047818791946308725,\n",
    "    'Coverage@40': 0.05965697240865026,\n",
    "    'Coverage@50': 0.07252050708426547,\n",
    "    'Coverage@60': 0.0837994034302759,\n",
    "    'Coverage@70': 0.09619686800894854,\n",
    "    'Coverage@80': 0.10868754660700969,\n",
    "    'Coverage@90': 0.12052572706935123,\n",
    "    'Coverage@100': 0.13143176733780762\n",
    "}\n",
    "\n",
    "root = 'results/KuaiRec/'\n",
    "results_path = root + 'results.csv'\n",
    "\n",
    "# Read the existing results CSV file\n",
    "df = pd.read_csv(results_path, index_col=0)\n",
    "df['UltraGCN'] = pd.Series(ultragcn_metrics)\n",
    "\n",
    "# Save the updated DataFrame back to CSV\n",
    "df.to_csv(results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca3e39-88cf-48da-9b27-8e4f28775735",
   "metadata": {
    "tags": []
   },
   "source": [
    "## UserKNN KuaiRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27a3f6e-f6a9-4a97-be10-23d094787bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import os\n",
    "\n",
    "class UserKNN:\n",
    "    def __init__(self, train_mat, K_neighbors=20):\n",
    "        \"\"\"\n",
    "        Initialize UserKNN with the training user-item interaction matrix.\n",
    "        \n",
    "        Args:\n",
    "            train_mat (sp.csr_matrix): User-item interaction matrix (users x items).\n",
    "            K_neighbors (int): Number of neighbors to consider for recommendations.\n",
    "        \"\"\"\n",
    "        self.train_mat = train_mat  # CSR sparse matrix\n",
    "        self.K_neighbors = K_neighbors\n",
    "        self.num_users, self.num_items = train_mat.shape\n",
    "        print(f\"Initializing UserKNN with {self.num_users} users, {self.num_items} items, K={K_neighbors}\")\n",
    "        # Compute user-user similarity matrix\n",
    "        self.user_sim = self.compute_user_similarity()\n",
    "\n",
    "    def compute_user_similarity(self):\n",
    "        \"\"\"\n",
    "        Compute cosine similarity between users based on the sparse interaction matrix.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: User-user similarity matrix (num_users x num_users).\n",
    "        \"\"\"\n",
    "        print(\"Computing user-user similarity matrix...\")\n",
    "        start_time = time.time()\n",
    "        # Cosine similarity on sparse matrix returns a dense array\n",
    "        sim = cosine_similarity(self.train_mat, dense_output=True)\n",
    "        # Set similarity of a user with themselves to -1 to exclude in neighbor selection\n",
    "        np.fill_diagonal(sim, -1)\n",
    "        print(f\"Similarity matrix computed in {time.time() - start_time:.2f} seconds\")\n",
    "        return sim\n",
    "\n",
    "    def get_top_k_neighbors(self, user_id):\n",
    "        \"\"\"\n",
    "        Get indices of the top K most similar users for a given user.\n",
    "        \n",
    "        Args:\n",
    "            user_id (int): User index.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Indices of top K neighbors.\n",
    "        \"\"\"\n",
    "        sim_scores = self.user_sim[user_id]\n",
    "        # Get top K indices (highest similarities), excluding the user themselves (set to -1)\n",
    "        top_k_indices = np.argsort(-sim_scores)[:self.K_neighbors]\n",
    "        return top_k_indices\n",
    "\n",
    "    def recommend(self, user_id, top_k=100):\n",
    "        \"\"\"\n",
    "        Generate top-k item recommendations for a user.\n",
    "        \n",
    "        Args:\n",
    "            user_id (int): User index.\n",
    "            top_k (int): Number of items to recommend.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Indices of top-k recommended items.\n",
    "        \"\"\"\n",
    "        # Get top K neighbors\n",
    "        neighbors = self.get_top_k_neighbors(user_id)\n",
    "        # Sum interactions of neighbors (sparse matrix sum along rows)\n",
    "        neighbor_interactions = self.train_mat[neighbors].sum(axis=0).A1  # Convert to 1D array\n",
    "        # Exclude items the user has already interacted with\n",
    "        user_interactions = self.train_mat[user_id].nonzero()[1]\n",
    "        candidate_items = np.setdiff1d(np.arange(self.num_items), user_interactions)\n",
    "        # Score candidates based on neighbor interactions\n",
    "        scores = neighbor_interactions[candidate_items]\n",
    "        # Get top-k items\n",
    "        top_k_indices = candidate_items[np.argsort(-scores)[:top_k]]\n",
    "        return top_k_indices\n",
    "\n",
    "    def evaluate(self, ground_truth_dict, other_dict, top_k_list=[100], is_validation=True):\n",
    "        \"\"\"\n",
    "        Evaluate UserKNN on validation or test data, matching kuairec_eval metrics.\n",
    "        \n",
    "        Args:\n",
    "            ground_truth_dict (dict): {user_id: [item_ids]} for ground truth.\n",
    "            other_dict (dict): {user_id: [item_ids]} for items to exclude (e.g., test items during validation).\n",
    "            top_k_list (list): List of k values for metrics (e.g., [100]).\n",
    "            is_validation (bool): True if evaluating validation, False for test.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Metrics like Recall@k, NDCG@k, Coverage@k.\n",
    "        \"\"\"\n",
    "        total_recall = {k: 0.0 for k in top_k_list}\n",
    "        total_ndcg = {k: 0.0 for k in top_k_list}\n",
    "        recommended_items = {k: set() for k in top_k_list}\n",
    "        total_items = self.num_items\n",
    "        user_count = 0\n",
    "\n",
    "        # Precompute IDCG for efficiency\n",
    "        idcg_cache = {}\n",
    "        def get_idcg(r_size, k):\n",
    "            cut = min(k, r_size)\n",
    "            if (cut, k) not in idcg_cache:\n",
    "                val = sum(1.0 / np.log2(i + 2) for i in range(cut))\n",
    "                idcg_cache[(cut, k)] = val\n",
    "            return idcg_cache[(cut, k)]\n",
    "\n",
    "        print(f\"Evaluating UserKNN on {'validation' if is_validation else 'test'} set...\")\n",
    "        start_time = time.time()\n",
    "        for u, relevant_set in ground_truth_dict.items():\n",
    "            if not relevant_set:\n",
    "                continue\n",
    "            other_set = other_dict.get(u, set())\n",
    "            # Get recommendations\n",
    "            rec_items = self.recommend(u, max(top_k_list))\n",
    "            # Exclude items in other_set (e.g., test items during validation)\n",
    "            rec_items = [item for item in rec_items if item not in other_set]\n",
    "            # Find positions of hits\n",
    "            hits_positions = [pos for pos, item in enumerate(rec_items) if item in relevant_set]\n",
    "\n",
    "            for k in top_k_list:\n",
    "                hits_count = sum(1 for pos in hits_positions if pos < k)\n",
    "                recall_k = hits_count / len(relevant_set)\n",
    "                total_recall[k] += recall_k\n",
    "\n",
    "                dcg = sum(1.0 / np.log2(pos + 2) for pos in hits_positions if pos < k)\n",
    "                idcg = get_idcg(len(relevant_set), k)\n",
    "                ndcg_k = dcg / idcg if idcg > 0 else 0.0\n",
    "                total_ndcg[k] += ndcg_k\n",
    "\n",
    "                recommended_items[k].update(rec_items[:k])\n",
    "            user_count += 1\n",
    "\n",
    "        if user_count == 0:\n",
    "            metrics = {f'Recall@{k}': 0.0 for k in top_k_list}\n",
    "            metrics.update({f'NDCG@{k}': 0.0 for k in top_k_list})\n",
    "            metrics.update({f'Coverage@{k}': 0.0 for k in top_k_list})\n",
    "        else:\n",
    "            metrics = {f'Recall@{k}': total_recall[k] / user_count for k in top_k_list}\n",
    "            metrics.update({f'NDCG@{k}': total_ndcg[k] / user_count for k in top_k_list})\n",
    "            metrics.update({f'Coverage@{k}': len(recommended_items[k]) / total_items for k in top_k_list})\n",
    "\n",
    "        print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6a3dc1-d7c8-459c-92b7-a1ef4890f62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KuaiRec data\n",
      "Building interaction matrix for 7176 users and 10728 items...\n",
      "Interaction matrix built with 936568 interactions\n",
      "Train Matrix computed\n",
      "Initializing UserKNN with 7176 users, 10728 items, K=1500\n",
      "Computing user-user similarity matrix...\n",
      "Similarity matrix computed in 1.67 seconds\n",
      "Evaluating UserKNN on validation set...\n",
      "Evaluation completed in 2.05 seconds\n",
      "Validation Metrics: {'Recall@100': 0.23811217463706705, 'NDCG@100': 0.16704731894823835, 'Coverage@100': 0.0267524235645041}\n"
     ]
    }
   ],
   "source": [
    "def build_interaction_matrix(adj_data):\n",
    "    \"\"\"\n",
    "    Build the user-item interaction matrix from adj_data.\n",
    "    \n",
    "    Args:\n",
    "        adj_data (dict): Contains rowptr_dict, col_dict, num_nodes_dict.\n",
    "    \n",
    "    Returns:\n",
    "        sp.csr_matrix: User-item interaction matrix.\n",
    "    \"\"\"\n",
    "    rowptr = adj_data['rowptr_dict'][('user', 'watches', 'video')]\n",
    "    col = adj_data['col_dict'][('user', 'watches', 'video')]\n",
    "    num_users = adj_data['num_nodes_dict']['user']\n",
    "    num_items = adj_data['num_nodes_dict']['video']\n",
    "    \n",
    "    print(f\"Building interaction matrix for {num_users} users and {num_items} items...\")\n",
    "    train_data = []\n",
    "    for u in range(num_users):\n",
    "        start = rowptr[u]\n",
    "        end = rowptr[u + 1]\n",
    "        items = col[start:end].tolist()\n",
    "        train_data.extend([(u, i) for i in items])\n",
    "    \n",
    "    train_mat = sp.coo_matrix(\n",
    "        (np.ones(len(train_data)), ([x[0] for x in train_data], [x[1] for x in train_data])),\n",
    "        shape=(num_users, num_items)\n",
    "    ).tocsr()\n",
    "    print(f\"Interaction matrix built with {len(train_data)} interactions\")\n",
    "    return train_mat\n",
    "\n",
    "# Load KuaiRec data\n",
    "with open(\"KuaiRec/adj_data.pkl\", \"rb\") as f:\n",
    "    adj_data = pickle.load(f)\n",
    "with open(\"KuaiRec/val_data.pkl\", \"rb\") as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open(\"KuaiRec/test_data.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "print(\"Loaded KuaiRec data\")\n",
    "\n",
    "# Build user-item interaction matrix\n",
    "train_mat = build_interaction_matrix(adj_data)\n",
    "print(\"Train Matrix computed\")\n",
    "# Initialize UserKNN\n",
    "userknn = UserKNN(train_mat, K_neighbors=1500)\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_metrics = userknn.evaluate(\n",
    "    ground_truth_dict=val_data,\n",
    "    other_dict=test_data,\n",
    "    top_k_list=[100],\n",
    "    is_validation=True\n",
    ")\n",
    "print(\"Validation Metrics:\", val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90a411fe-11b1-48da-88fe-11ae4ab85c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating UserKNN on test set...\n",
      "Evaluation completed in 2.29 seconds\n",
      "Test Metrics: {'Recall@1': 0.013078878816451046, 'Recall@2': 0.021195434520410204, 'Recall@3': 0.03047994523955555, 'Recall@5': 0.048304170195103246, 'Recall@10': 0.065279947994868, 'Recall@20': 0.10188392715789932, 'Recall@30': 0.13878712499612897, 'Recall@40': 0.1707852716654582, 'Recall@50': 0.18684843349166558, 'Recall@60': 0.20118618388505702, 'Recall@70': 0.21322902820111223, 'Recall@80': 0.22188521953869586, 'Recall@90': 0.2289625564146559, 'Recall@100': 0.23377915539117006, 'NDCG@1': 0.5506732813607371, 'NDCG@2': 0.468696676567592, 'NDCG@3': 0.45417414452862837, 'NDCG@5': 0.44020124249000414, 'NDCG@10': 0.35385302861633733, 'NDCG@20': 0.30207054610804834, 'NDCG@30': 0.289711071974637, 'NDCG@40': 0.2880385713835392, 'NDCG@50': 0.2826861154315943, 'NDCG@60': 0.279640267445158, 'NDCG@70': 0.27816820072313847, 'NDCG@80': 0.277117105820763, 'NDCG@90': 0.27678582620150727, 'NDCG@100': 0.2747678931602998, 'Coverage@1': 0.0012117822520507084, 'Coverage@2': 0.0015846383296047725, 'Coverage@3': 0.0018642803877703207, 'Coverage@5': 0.0025167785234899327, 'Coverage@10': 0.005313199105145414, 'Coverage@20': 0.00848247576435496, 'Coverage@30': 0.011465324384787472, 'Coverage@40': 0.013516032811334825, 'Coverage@50': 0.015846383296047727, 'Coverage@60': 0.018642803877703208, 'Coverage@70': 0.021532438478747203, 'Coverage@80': 0.024049217002237135, 'Coverage@90': 0.026006711409395974, 'Coverage@100': 0.02703206562266965}\n"
     ]
    }
   ],
   "source": [
    "test_metrics = userknn.evaluate(\n",
    "    ground_truth_dict=test_data,\n",
    "    other_dict=val_data,\n",
    "    top_k_list=[1,2,3,5,10,20,30,40,50,60,70,80,90,100],\n",
    "    is_validation=False\n",
    ")\n",
    "print(\"Test Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94378cc3-326a-439c-8c6a-df943a16e2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "w2v-exp",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "w2v-exp",
   "language": "python",
   "name": "w2v-exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
