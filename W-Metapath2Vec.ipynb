{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b9303b0-87d4-4d5a-b181-afb5bb9c7690",
   "metadata": {},
   "source": [
    "### Weighted Metapath2Vec KuaiRec Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6bd1823-40e0-4cb3-8bbf-370087489763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from scipy.stats import wilcoxon\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn.models import MetaPath2Vec\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.index import index2ptr\n",
    "from torch_geometric.typing import EdgeType, NodeType, OptTensor\n",
    "from torch_geometric.utils import sort_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f677b2b-d695-4ec9-9658-27d3ee6d1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 29s, sys: 1.4 s, total: 27min 31s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70180360, 70242600)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Best W2V video/track embeddings config will be used as initialization for Metapath2vec\n",
    "train = pd.read_parquet('KuaiRec/train_sequences.parquet')\n",
    "sequences = train['video_id'].apply(lambda x: list(map(str, x))).tolist()\n",
    "val = np.load('KuaiRec/val_pairs.npy')\n",
    "model = Word2Vec(\n",
    "            vector_size=50,\n",
    "            window=15,\n",
    "            workers=multiprocessing.cpu_count(),\n",
    "            sg=1,\n",
    "            min_count=1,\n",
    "            compute_loss=True,\n",
    "        )\n",
    "model.build_vocab(sequences)\n",
    "model.train(\n",
    "            corpus_iterable=sequences,\n",
    "            total_examples=len(sequences),\n",
    "            epochs=75,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bdf5a61-c735-4f30-9d54-8080ca287eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_social = pd.read_csv('KuaiRec/social_network.csv')\n",
    "df_watch = pd.read_csv('KuaiRec/big_matrix.csv')\n",
    "df_watch = df_watch[df_watch['watch_ratio'] >= 2].sort_values(['user_id', 'time'])\n",
    "\n",
    "unique_users = df_watch['user_id'].unique()\n",
    "unique_videos = df_watch['video_id'].unique()\n",
    "\n",
    "user2idx = {uid: i for i, uid in enumerate(unique_users)}\n",
    "video2idx = {vid: i for i, vid in enumerate(unique_videos)}\n",
    "idx2video = {i: vid for vid, i in video2idx.items()}\n",
    "\n",
    "user_col = df_watch['user_id'].map(user2idx).values\n",
    "video_col = df_watch['video_id'].map(video2idx).values\n",
    "edge_index_uv = np.vstack([user_col, video_col])\n",
    "edge_weight_uv = torch.tensor(df_watch['watch_ratio'].values, dtype=torch.float)\n",
    "\n",
    "data = HeteroData()\n",
    "data['user'].num_nodes = len(user2idx)\n",
    "data['video'].num_nodes = len(video2idx)\n",
    "\n",
    "data['user', 'watches', 'video'].edge_index = torch.from_numpy(edge_index_uv)\n",
    "data['user', 'watches', 'video'].edge_weight = edge_weight_uv\n",
    "edge_index_vu = np.flip(edge_index_uv, axis=0).copy(order='C')\n",
    "data['video', 'watched_by', 'user'].edge_index = torch.from_numpy(edge_index_vu)\n",
    "data['video', 'watched_by', 'user'].edge_weight = edge_weight_uv\n",
    "\n",
    "df_social['friend_list'] = df_social['friend_list'].apply(lambda x: x.strip('[]').split(','))\n",
    "df_social = df_social.explode('friend_list').dropna()\n",
    "df_social['friend_list'] = df_social['friend_list'].astype(int)\n",
    "\n",
    "# Build user-user edge index\n",
    "userA = df_social['user_id'].map(user2idx).values\n",
    "userB = df_social['friend_list'].map(user2idx).values\n",
    "\n",
    "edge_index_uu = np.vstack([userA, userB])\n",
    "data['user', 'follows', 'user'].edge_index = torch.tensor(edge_index_uu, dtype=torch.long)\n",
    "w2v_dim = model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c29f27c-7cf4-4f69-8d8b-ee973f26ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zero tensor for all video embeddings & update them with W2V trained ones\n",
    "model_vocab = list(model.wv.index_to_key)\n",
    "video_emb = np.zeros((len(video2idx), w2v_dim), dtype=np.float32)\n",
    "for vid, idx in video2idx.items():\n",
    "    if str(vid) in model_vocab:\n",
    "        video_emb[idx] = model.wv[str(vid)]\n",
    "\n",
    "# Set data video embeddings to the W2V embeddings\n",
    "data['video'].x = torch.tensor(video_emb, dtype=torch.float32)\n",
    "\n",
    "num_users = len(user2idx)\n",
    "\n",
    "user_to_videos = defaultdict(list)\n",
    "for _, row in df_watch.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    video_id_str = str(row['video_id'])  # Convert to string for Word2Vec keys\n",
    "    user_to_videos[user_id].append(video_id_str)\n",
    "\n",
    "# Initialize user embeddings to their average video embeddings from training watch history\n",
    "user_emb = torch.zeros((num_users, w2v_dim), dtype=torch.float32)\n",
    "for uid, u_idx in user2idx.items():\n",
    "    vids_watched = user_to_videos[uid]\n",
    "    if not vids_watched:\n",
    "        continue  # user_emb remains zero if no videos\n",
    "\n",
    "    sum_vec = np.zeros(w2v_dim, dtype=np.float32)\n",
    "    count = 0\n",
    "    for vid_str in vids_watched:\n",
    "        if vid_str in model.wv:  # If the video ID is in the W2V vocab\n",
    "            sum_vec += model.wv[vid_str]\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        user_emb[u_idx] = torch.tensor(sum_vec / count, dtype=torch.float32)\n",
    "\n",
    "# Set data user embeddings to the user's average W2V video embeddings\n",
    "data['user'].x = user_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be87df0b-382b-49db-b5f7-7bec64936ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path_uuv = [\n",
    "    ('user', 'follows', 'user'),\n",
    "    ('user', 'watches', 'video'),\n",
    "    ('video', 'watched_by', 'user')\n",
    "]\n",
    "with open('KuaiRec/val_data.pkl', 'rb') as f:\n",
    "    val_data = pickle.load(f)\n",
    "with open('KuaiRec/test_data.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b308e2-b5b6-4e7e-8059-13ea4eb9303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch: 1, Loss: 40.038991928100586, NDCG@100: 0.05127449892627967\n",
      "Epoch: 2, Loss: 29.6527441740036, NDCG@100: 0.05152631456902219\n",
      "Epoch: 3, Loss: 22.97763967514038, NDCG@100: 0.05212739985338954\n",
      "Epoch: 4, Loss: 18.724189281463623, NDCG@100: 0.05317924935692166\n",
      "Epoch: 5, Loss: 15.866265714168549, NDCG@100: 0.05501802231604555\n",
      "Epoch: 6, Loss: 13.939627826213837, NDCG@100: 0.05709777941487168\n",
      "Epoch: 7, Loss: 12.49271297454834, NDCG@100: 0.059885960806138124\n",
      "Epoch: 8, Loss: 11.340573251247406, NDCG@100: 0.062459960644255366\n",
      "Epoch: 9, Loss: 10.410344183444977, NDCG@100: 0.06481431402392643\n",
      "Epoch: 10, Loss: 9.619074821472168, NDCG@100: 0.06762018129887763\n",
      "Epoch: 11, Loss: 9.052366435527802, NDCG@100: 0.06989110751361328\n",
      "Epoch: 12, Loss: 8.596085727214813, NDCG@100: 0.07354039883199161\n",
      "Epoch: 13, Loss: 8.316554844379425, NDCG@100: 0.07662213557197808\n",
      "Epoch: 14, Loss: 7.986779034137726, NDCG@100: 0.08013454615268198\n",
      "Epoch: 15, Loss: 7.719898879528046, NDCG@100: 0.0818118206663627\n",
      "Epoch: 16, Loss: 7.50844618678093, NDCG@100: 0.08366400080269631\n",
      "Epoch: 17, Loss: 7.357764631509781, NDCG@100: 0.08506319950897832\n",
      "Epoch: 18, Loss: 7.259468227624893, NDCG@100: 0.08529512093235908\n",
      "Epoch: 19, Loss: 7.084767043590546, NDCG@100: 0.08614276370499772\n",
      "Epoch: 20, Loss: 7.08292555809021, NDCG@100: 0.08706616804766017\n",
      "Epoch: 21, Loss: 6.889943093061447, NDCG@100: 0.08682667312462801\n",
      "Epoch: 22, Loss: 6.928675591945648, NDCG@100: 0.08768311225913832\n",
      "Epoch: 23, Loss: 6.697559654712677, NDCG@100: 0.08708746147470765\n",
      "Epoch: 24, Loss: 6.780552983283997, NDCG@100: 0.08669775570612322\n",
      "Epoch: 25, Loss: 6.562965631484985, NDCG@100: 0.08837878538498152\n",
      "Epoch: 26, Loss: 6.507822632789612, NDCG@100: 0.08820535705572534\n",
      "Epoch: 27, Loss: 6.440188527107239, NDCG@100: 0.08792043208038432\n",
      "Epoch: 28, Loss: 6.384106993675232, NDCG@100: 0.08726325688577886\n",
      "Epoch: 29, Loss: 6.341649502515793, NDCG@100: 0.08694807148684615\n",
      "Epoch: 30, Loss: 6.322465568780899, NDCG@100: 0.08595784023349395\n",
      "Epoch: 31, Loss: 6.248653799295425, NDCG@100: 0.08584809806720312\n",
      "Epoch: 32, Loss: 6.189191490411758, NDCG@100: 0.0854450693183887\n",
      "Epoch: 33, Loss: 6.154016822576523, NDCG@100: 0.0860634407752112\n",
      "Epoch: 34, Loss: 6.1229954063892365, NDCG@100: 0.0863134434433467\n",
      "Epoch: 35, Loss: 6.078121900558472, NDCG@100: 0.08511829249628416\n",
      "Epoch: 36, Loss: 6.050615221261978, NDCG@100: 0.08453369212159195\n",
      "Epoch: 37, Loss: 6.04336479306221, NDCG@100: 0.08361179499104114\n",
      "Training complete.\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1411/1411 [00:01<00:00, 1211.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "{'HR@100': 0.1431752373353853, 'NDCG@100': 0.08361179499104114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "from models.Wmetapath2vec import WMetaPath2VecRecommender\n",
    "\n",
    "# Example configuration (adjust as needed)\n",
    "config = {\n",
    "    'walk_length': 10,\n",
    "    'context_size': 3,\n",
    "    'walks_per_node': 2,\n",
    "    'num_negative_samples': 5,\n",
    "    'lr': 0.005,\n",
    "    'embedding_dim': 50,\n",
    "    'max_epochs': 40,\n",
    "    'patience': 12,\n",
    "    'batch_size': 512,\n",
    "    'k': [100],           # Evaluate at top-100\n",
    "    'device': 'cpu',      # or 'cuda' if GPU available\n",
    "    'verbose': True,\n",
    "}\n",
    "\n",
    "# Instantiate the recommender with the configuration\n",
    "recommender = WMetaPath2VecRecommender(config)\n",
    "\n",
    "# Fit the model (this will train the model using your data and metapath)\n",
    "print(\"Starting training...\")\n",
    "recommender.fit(\n",
    "    data=data,\n",
    "    metapath=meta_path_uuv,  # e.g. [('user', 'watches', 'video'), ('video', 'watched_by', 'user'), ('user', 'follows', 'user')]\n",
    "    user2idx=user2idx,\n",
    "    video2idx=video2idx,\n",
    "    idx2video=idx2video,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data\n",
    ")\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_metrics = recommender.evaluate(\n",
    "    user_emb=recommender.user_emb,\n",
    "    video_emb=recommender.video_emb,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    user2idx=user2idx,\n",
    "    video2idx=video2idx,\n",
    "    idx2video=idx2video,\n",
    "    top_k=[100],\n",
    "    is_validation=True,\n",
    "    progress_bar=True,\n",
    "    return_per_user=False\n",
    ")\n",
    "\n",
    "print(\"Validation metrics:\")\n",
    "print(val_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c77802-d538-4625-a78e-3692a1163144",
   "metadata": {},
   "source": [
    "### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e98d7b-1cb8-4280-8993-67ce7ec0ed21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparam search: 100%|█████████▉| 299/300 [33:03:21<37:19, 2239.07s/it]   "
     ]
    }
   ],
   "source": [
    "from models.Wmetapath2vec import WMetaPath2VecRecommender\n",
    "\n",
    "# Hyperparameter ranges:\n",
    "WALK_LENGTHS = [4, 10, 15, 25]\n",
    "CONTEXT_SIZES = [2, 3, 6, 10, 15]\n",
    "WALKS_PER_NODE = [3, 5, 10, 15, 30]\n",
    "NUM_NEG_SAMPLES = [3, 6, 10]\n",
    "LEARNING_RATES = [0.005]\n",
    "\n",
    "# Other hyperparameters:\n",
    "MAX_EPOCHS = 40\n",
    "PATIENCE = 12  # Early stopping patience\n",
    "BATCH_SIZE = 512\n",
    "w2v_dim = 50\n",
    "K = [1, 5, 10, 20, 40, 60, 80, 100]\n",
    "\n",
    "# Total number of configurations:\n",
    "all_configs = list(itertools.product(WALK_LENGTHS, CONTEXT_SIZES, WALKS_PER_NODE, NUM_NEG_SAMPLES, LEARNING_RATES))\n",
    "total_configs = len(all_configs)\n",
    "\n",
    "# Initialize tracking variables:\n",
    "best_ndcg100 = -1.0\n",
    "best_config = None\n",
    "best_val_metrics = None\n",
    "\n",
    "# Loop over hyperparameter combinations:\n",
    "for (wl, cs, wpn, neg, lr) in tqdm(all_configs, total=total_configs, desc=\"Hyperparam search\"):\n",
    "    config_dict = {\n",
    "        'walk_length': wl,\n",
    "        'context_size': cs,\n",
    "        'walks_per_node': wpn,\n",
    "        'num_negative_samples': neg,\n",
    "        'lr': lr,\n",
    "        'embedding_dim': w2v_dim,\n",
    "        'max_epochs': MAX_EPOCHS,\n",
    "        'patience': PATIENCE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'k': K,\n",
    "        'device': 'cpu',  # or 'cuda' if you use GPU\n",
    "        'verbose': False,\n",
    "    }\n",
    "    \n",
    "    # Instantiate your recommender with the configuration:\n",
    "    recommender = WMetaPath2VecRecommender(config_dict)\n",
    "    \n",
    "    try:\n",
    "        # Fit the model using your data and metapath (e.g., meta_path_uuv)\n",
    "        recommender.fit(\n",
    "            data=data,\n",
    "            metapath=meta_path_uuv,\n",
    "            user2idx=user2idx,\n",
    "            video2idx=video2idx,\n",
    "            idx2video=idx2video,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation data:\n",
    "        val_metrics = recommender.evaluate(\n",
    "            user_emb=recommender.user_emb,\n",
    "            video_emb=recommender.video_emb,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            user2idx=user2idx,\n",
    "            video2idx=video2idx,\n",
    "            idx2video=idx2video,\n",
    "            top_k=K,\n",
    "            is_validation=True,\n",
    "            progress_bar=False,\n",
    "            return_per_user=False\n",
    "        )\n",
    "        \n",
    "        # Use NDCG@100 as the key for model selection. Adjust the key name if needed.\n",
    "        ndcg_100 = val_metrics.get('NDCG@100', 0.0)\n",
    "        if ndcg_100 > best_ndcg100:\n",
    "            best_ndcg100 = ndcg_100\n",
    "            best_config = copy.deepcopy(config_dict)\n",
    "            best_val_metrics = copy.deepcopy(val_metrics)\n",
    "            \n",
    "    except Exception as e:\n",
    "        continue\n",
    "    finally:\n",
    "        del recommender\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Hyperparameter search complete!\")\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"Best validation metrics:\", best_val_metrics)\n",
    "print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe2383-72eb-4ac6-8e20-c45120d23421",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90dfba-060f-47c9-9d91-ab36fcc78d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc54bc3-92da-46b5-9ad9-4bea3979f72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2707b597-f4dd-4111-a432-41a4f8105c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "Error in training: \n",
      "\n",
      "==============================\n",
      "Search complete!\n",
      "Best HR@100: 0.2490927854183785  NDCG@100: 0.16127212701619853\n",
      "Best config: {'walk_length': 10, 'context_size': 3, 'walks_per_node': 5, 'num_negative_samples': 5, 'lr': 0.01}\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "WALK_LENGTHS = [2, 4, 10, 15, 25]\n",
    "CONTEXT_SIZES = [2, 3, 6, 10, 15]\n",
    "WALKS_PER_NODE = [1, 2, 5, 10, 15]\n",
    "NUM_NEG_SAMPLES = [1, 3, 5, 8, 10]\n",
    "LEARNING_RATES = [0.005]\n",
    "\n",
    "MAX_EPOCHS = 40\n",
    "PATIENCE = 12  # Early stopping patience\n",
    "\n",
    "def run_wmetapath2vec_experiment(\n",
    "    data,  # HeteroData\n",
    "    evaluate_user_reco,  # your evaluation function\n",
    "    user2idx, video2idx, idx2video, val_data, test_data,\n",
    "    walk_length, context_size, walks_per_node, num_negative_samples,\n",
    "    lr, w2v_dim\n",
    "):\n",
    "    \"\"\"Train WMetaPath2Vec with given hyperparams, return best HR@100.\"\"\"    \n",
    "    try:\n",
    "        # 1) Build MetaPath2Vec model with these hyperparams\n",
    "        model_metapath = WeightedMetaPath2Vec(\n",
    "            data.edge_index_dict,\n",
    "            edge_weight_dict,\n",
    "            embedding_dim=w2v_dim,\n",
    "            metapath=meta_path_uuv,\n",
    "            walk_length=walk_length,\n",
    "            context_size=context_size,\n",
    "            walks_per_node=walks_per_node,\n",
    "            num_negative_samples=num_negative_samples,\n",
    "            sparse=True\n",
    "        ).to('cpu')\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {e}\")\n",
    "        return (None, None)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Copy in user & video inits:\n",
    "        model_metapath('video').data.copy_(data['video'].x.to('cpu'))\n",
    "        model_metapath('user').data.copy_(data['user'].x.to('cpu'))\n",
    "\n",
    "    loader = model_metapath.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(model_metapath.parameters(), lr=lr)\n",
    "\n",
    "    best_hr = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    def train_one_epoch():\n",
    "        model_metapath.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            pos_rw, neg_rw = pos_rw.to('cpu'), neg_rw.to('cpu')\n",
    "            optimizer.zero_grad()\n",
    "            loss = model_metapath.loss(pos_rw, neg_rw)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss\n",
    "\n",
    "    best_hr_so_far = 0.0\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        loss_val = train_one_epoch()\n",
    "\n",
    "        # Evaluate on validation:\n",
    "        model_metapath.eval()\n",
    "        with torch.no_grad():\n",
    "            user_emb = model_metapath('user').cpu().detach().numpy()\n",
    "            video_emb = model_metapath('video').cpu().detach().numpy()\n",
    "        \n",
    "        val_metrics = evaluate_user_reco(\n",
    "            user_emb=user_emb,\n",
    "            video_emb=video_emb,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            user2idx=user2idx,\n",
    "            video2idx=video2idx,\n",
    "            idx2video=idx2video,\n",
    "            is_validation=True,\n",
    "            top_k=100,\n",
    "            progress_bar=False\n",
    "        )\n",
    "        hr100 = val_metrics.get('HR@100', 0.0)\n",
    "        ndcg100 = val_metrics.get(\"nDCG@100\", 0.0)\n",
    "\n",
    "        # Early stopping check:\n",
    "        if hr100 > best_hr_so_far:\n",
    "            best_hr_so_far = hr100\n",
    "            best_ndcg_so_far = ndcg100\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            break\n",
    "\n",
    "    return (best_hr_so_far, best_ndcg_so_far)\n",
    "\n",
    "search_results = []\n",
    "best_hr_overall = 0.0\n",
    "best_config = None\n",
    "\n",
    "for wl, cs, wpn, neg, learning_rate in itertools.product(\n",
    "    WALK_LENGTHS,\n",
    "    CONTEXT_SIZES,\n",
    "    WALKS_PER_NODE,\n",
    "    NUM_NEG_SAMPLES,\n",
    "    LEARNING_RATES\n",
    "):\n",
    "    \n",
    "    hr100, ndcg100 = run_metapath2vec_experiment(\n",
    "        data=data,\n",
    "        evaluate_user_reco=evaluate_user_reco,\n",
    "        user2idx=user2idx,\n",
    "        video2idx=video2idx,\n",
    "        idx2video=idx2video,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        walk_length=wl,\n",
    "        context_size=cs,\n",
    "        walks_per_node=wpn,\n",
    "        num_negative_samples=neg,\n",
    "        lr=learning_rate,\n",
    "        w2v_dim=w2v_dim\n",
    "    )\n",
    "    \n",
    "    config = {\n",
    "        'walk_length': wl,\n",
    "        'context_size': cs,\n",
    "        'walks_per_node': wpn,\n",
    "        'num_negative_samples': neg,\n",
    "        'lr': learning_rate\n",
    "    }\n",
    "    search_results.append((config, hr100))\n",
    "\n",
    "    if ((hr100 is not None) and (hr100 > best_hr_overall)):\n",
    "        best_hr_overall = hr100\n",
    "        best_ndcg_overall = ndcg100\n",
    "        best_config = copy.deepcopy(config)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"Search complete!\")\n",
    "print(\"Best HR@100:\", best_hr_overall, \" NDCG@100:\", best_ndcg_overall)\n",
    "print(\"Best config:\", best_config)\n",
    "print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16280a8b-a5e6-4f0e-b3f8-616b8cb32174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best metapath2vec model:\n",
    "model_metapath = WeightedMetaPath2Vec(\n",
    "    data.edge_index_dict,\n",
    "    edge_weight_dict,\n",
    "    embedding_dim=50,\n",
    "    metapath=meta_path_uuv,\n",
    "    walk_length=10,\n",
    "    context_size=3,\n",
    "    walks_per_node=5,\n",
    "    num_negative_samples=5,\n",
    "    sparse=True\n",
    ").to('cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_metapath('video').data.copy_(data['video'].x.to('cpu'))\n",
    "    model_metapath('user').data.copy_(data['user'].x.to('cpu'))\n",
    "\n",
    "loader = model_metapath.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(\n",
    "    [p for p in model_metapath.parameters()],\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "def train_one_epoch():\n",
    "    model_metapath.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        pos_rw, neg_rw = pos_rw.to('cpu'), neg_rw.to('cpu')\n",
    "        optimizer.zero_grad()\n",
    "        loss = model_metapath.loss(pos_rw, neg_rw)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    epoch_loss = train_one_epoch()\n",
    "    print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate on validation:\n",
    "    model_metapath.eval()\n",
    "    user_emb = model_metapath('user').cpu().detach().numpy()\n",
    "    video_emb = model_metapath('video').cpu().detach().numpy()\n",
    "\n",
    "    val_metrics = evaluate_user_reco(\n",
    "        user_emb=user_emb,\n",
    "        video_emb=video_emb,\n",
    "        val_data=val_data,\n",
    "        test_data=test_data,\n",
    "        user2idx=user2idx,\n",
    "        video2idx=video2idx,\n",
    "        idx2video=idx2video,\n",
    "        is_validation=True,   # i.e., we're evaluating on val_data\n",
    "        top_k=100\n",
    "    )\n",
    "    print(\"Val metrics:\", val_metrics)\n",
    "    \n",
    "user_emb_meta = model_metapath('user').cpu().detach().numpy()\n",
    "video_emb_meta = model_metapath('video').cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2639ff9f-cee0-4934-9574-bd04a535f233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1411/1411 [00:01<00:00, 716.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'HR@100': 0.27245404188231237, 'nDCG@100': 0.23753742950303666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run both on test set\n",
    "meta_test_metrics = evaluate_user_reco(\n",
    "    user_emb=user_emb_meta,\n",
    "    video_emb=video_emb_meta,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,    # so we ignore val items if they appear\n",
    "    user2idx=user2idx,\n",
    "    video2idx=video2idx,\n",
    "    idx2video=idx2video,    # e.g., {0: 'vid_a', 1: 'vid_b', ...}\n",
    "    is_validation=False,\n",
    "    top_k=100,\n",
    "    return_per_user=True\n",
    ")\n",
    "print(meta_test_metrics[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c48dbf-d4a2-4986-ac63-a1e6a8e65fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "w2v-exp",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "w2v-exp",
   "language": "python",
   "name": "w2v-exp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
